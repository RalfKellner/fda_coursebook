
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Dimensionality reduction &#8212; Financial Data Analytics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '11_DimensionalityReduction';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Clustering" href="10_Clustering.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_Introduction.html">
  
  
  
  
  
  
    <p class="title logo__title">Financial Data Analytics</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_Introduction.html">
                    Financial Data Analytics
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_PythonIntroduction.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_DataAccess.html">Access to data</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_DescriptiveAnalysis.html">Descriptive analysis of data</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_SupervisedLearning.html">The analysis of dependent variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_LinearRegression.html">The multiple linear regression model</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Classification.html">Classification</a></li>


<li class="toctree-l1"><a class="reference internal" href="07_ModellAccuracy.html">Evaluation of trained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_ModelComplexity.html">Model complexity</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Regularization.html">Regularization</a></li>

<li class="toctree-l1"><a class="reference internal" href="10_Clustering.html">Clustering</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Dimensionality reduction</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F11_DimensionalityReduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/11_DimensionalityReduction.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Dimensionality reduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis">Principal Component Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-components-of-financial-returns">Principal components of financial returns</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-notes">Technical notes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">Notation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-to-singular-value-decomposition">Relation to singular value decomposition</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dimensionality-reduction">
<h1>Dimensionality reduction<a class="headerlink" href="#dimensionality-reduction" title="Link to this heading">#</a></h1>
<p>Dimensionality reduction is about generating <span class="math notranslate nohighlight">\(q \leq p\)</span> new variables from <span class="math notranslate nohighlight">\(p\)</span> variables of a data set <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times p}\)</span>. One of the motivations by dimensionality reduction techniques is to present data in a lower dimensional space and lose as little information is possible. The desire for lower dimensional representations (this just means we want a lower amount of variables) is that many statistical models easier trained for lower dimensional data.</p>
<p>We are going to take a look at principal component analysis (PCA) to learn about a popular dimensionality reduction technique. While it only uses linear relationships between variables to create lower dimensional representations of the data, it has nice mathematical properties and is interpretable. Due to the latter, it can be employed for important tasks in the field of financial markets, i.e., creating systematic factors and identifying risk contributors in a portfolio.</p>
<p>The math behind PCA is really interesting and elegant. However, beyond the scope of this course, given the different mathematical background of all students. I attach technical notes to the end of this chapter which are only for interested readers. They are not important for the course’s exam.</p>
<section id="principal-component-analysis">
<h2>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Link to this heading">#</a></h2>
<p>As an introduction to PCA, we focus on its intuition and some important definitions. Given the data matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times p}\)</span> whose rows are observations and whose columns are variables, PCA creates up to <span class="math notranslate nohighlight">\(p\)</span> vectors <span class="math notranslate nohighlight">\(\mathbb{w}_j\)</span>. Each of these vectors are called principal components. We assume that <span class="math notranslate nohighlight">\(X\)</span> is mean-centered. This means for each value we estimate and subtract the expected value. Furthermore, all principal components are unit vectors, which means they are normalized such that their length is equal to one. In addition, they are independent of each other which means that their dot product is equal to zero.</p>
<p>PCA is about the variation in the data. As data is mean-centered, the variation in the data is the sum of squared values of <span class="math notranslate nohighlight">\(X\)</span>, i.e., <span class="math notranslate nohighlight">\(TSS = \sum_{i=1}^n \sum_{j=1}^p x_{ij}^2\)</span>. The principal components are used to create new variables which capture as much variation of the data as possible. Given a principal component <span class="math notranslate nohighlight">\(\mathbf{w}_j \in \mathbb{R}^p\)</span>, principal component scores are determined by:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{z}_j = X \mathbf{w}_j 
\]</div>
<p>This is also called a <em>projection</em>. For each observation <span class="math notranslate nohighlight">\(i\)</span> (row) in the data set, the principal component score is determined as a weighted sum of the original observations from all variables. The weights are elements of the principal component vector.</p>
<div class="math notranslate nohighlight">
\[
z_{ij} = x_{i1} w_{1j} + x_{i2} w_{2j} + \cdots + x_{ip} w_{pj}
\]</div>
<p>The ratio of the variance of a principal component to the overall level of variation is given by:</p>
<div class="math notranslate nohighlight">
\[
PSS_j = \frac{SS_j}{TSS} = \frac{\sum_{i=1}^n z_{ij}^2}{\sum_{i = 1}^n \sum_{j = 1}^p x_{ij}^2}
\]</div>
<p>Note, understanding variation is understanding when observations deviate from their expected value. Assume, in an extreme example, <span class="math notranslate nohighlight">\(PSS_j\)</span> to be a high number, e.g., 80%. This means if we understand under which scenarios scores from the first principal component deviate to a large extend from their expected value, we understand where a large part of the overall variation comes from.</p>
<p>Without further explanations, note the following characteristics for PCA. The variance of principal components decreases. Thus, the first principal components usually capture a large ratio of the overall variation in the data. Another helpful information with this respect is to understand that principal components point into different directions of variation in the data.</p>
<p>Take a look in the scatter plot below which visualizes a data set with two variables. With PCA up to two principal components can be determined. We observe that the values of <span class="math notranslate nohighlight">\(x_1\)</span> vary more than the ones of <span class="math notranslate nohighlight">\(x_2\)</span>. Furthermore, a positive relation seems to exists between both variables. The two arrows represent the vectors of the principal components (scaled by the variance they capture, i.e., longer vector means more variation is captured by this component). You should be able to identify that the longer vector is along the axis of the highest level of variation. The second vector has a 90 degree angle to the first vector (this is due to their orthogonality) and heads into the other direction of variation in the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">covariance_hat</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">covariance_hat</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Scatter Points&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">vector</span><span class="p">,</span> <span class="n">scale</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">eigenvectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">vector</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">scale</span><span class="p">,</span> <span class="n">vector</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">scale</span><span class="p">,</span> 
                <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Enhance plot aesthetics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Scatter Plot with Principal Components&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/046073528aedd256745ff30e313d932efd7cb1b26d518d809d0b6cf505f2c9f9.png" src="_images/046073528aedd256745ff30e313d932efd7cb1b26d518d809d0b6cf505f2c9f9.png" />
</div>
</div>
<p>Variation in the data stems from the individual variance of each variable and their relationship. Given the relationship is linear, its strength can be quantified by covariance. If we take a look at the covariance matrix from the data above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">covariance_hat</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2.0038 0.4957]
 [0.4957 1.0412]]
</pre></div>
</div>
</div>
</div>
<p>we observe that the variance of <span class="math notranslate nohighlight">\(x_1\)</span> is about twice as much as the one from <span class="math notranslate nohighlight">\(x_2\)</span>. In addition, the overall variation is increased by a positive covariance. If we take a look at the first principal component:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.921  0.3895]
</pre></div>
</div>
</div>
</div>
<p>we observe that the weight for <span class="math notranslate nohighlight">\(x_1\)</span> is higher than for <span class="math notranslate nohighlight">\(x_2\)</span>. In this example, the first principle component scores are calculated by:</p>
<div class="math notranslate nohighlight">
\[
z_{i1} = x_{i1} w_{11} + x_{i2} w_{21} = 0.9166 x_{i1}  + 0.3998 x_{i2} 
\]</div>
<p>You can see, the variable with a higher level of variation (<span class="math notranslate nohighlight">\(x_1\)</span>) gets a higher weight, such that <span class="math notranslate nohighlight">\(z_{i1}\)</span> also varies to a large extent. If we take a look the second principal component:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.3895  0.921 ]
</pre></div>
</div>
</div>
</div>
<p>we observe a smaller weight for <span class="math notranslate nohighlight">\(x_1\)</span>. Furthermore, the signs are different which offsets variation from the two variables with a positive linear relationship. Visualizing the variation of the scores for the first and the second principal component, we clearly can see that the first principal component varies to a larger extent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$z_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$z_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Principal Components&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b3dc0c1b9c40af3ae1a4a8f32e86d01f3421d85d41c740e6fba79f61ca5aa8d1.png" src="_images/b3dc0c1b9c40af3ae1a4a8f32e86d01f3421d85d41c740e6fba79f61ca5aa8d1.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(PSS_1 = 0.75\)</span> which tells us that 75% from the overall variation in the data is captured by the first principal component. Furthermore, as both weights from the first principal component are positive, we know that if the score for the first principal component is positive, that the observations of <span class="math notranslate nohighlight">\(x_1, x_2\)</span> likely have the same sign. This can not said with certainty, however, the latter sentence highlights a characteristic of dimensionality reduction. From the lower representation, we can say something about the original data without knowing the original data. Not with certainty, however, on average we should be write with such an inference.</p>
<p>Another characteristic for dimensionality techniques is that the characteristics in the lower dimension are as close as possible as in the original dimension. For instance, we can determine how (dis-)similar observations are in the two dimensional space. For our example, we quantify this by the euclidean distance. If we determine the distances between three random observations by pairs of <span class="math notranslate nohighlight">\((x_{i1}, x_{i2})\)</span> or <span class="math notranslate nohighlight">\(z_{i1}\)</span>, respectively, we observe below that the relationship of similarity remains the same. In this example, observations 1 and 20 are more similar to each other than 1 and 200. This is true if we determine the euclidean distance in two dimensions as well as in one dimension which is represented by the first principal component.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">ecl_dist</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distance between the first and 20th samples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ecl_dist</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">19</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distance between the first and 200th samples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ecl_dist</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">199</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distance between the first and 20th samples of the first principal component score:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">z1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">z1</span><span class="p">[</span><span class="mi">19</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distance between the first and 200th samples of the first principal component score:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">z1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">z1</span><span class="p">[</span><span class="mi">199</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Distance between the first and 20th samples:
0.97
Distance between the first and 200th samples:
2.96
Distance between the first and 20th samples of the first principal component score:
0.25
Distance between the first and 200th samples of the first principal component score:
2.46
</pre></div>
</div>
</div>
</div>
</section>
<section id="principal-components-of-financial-returns">
<h2>Principal components of financial returns<a class="headerlink" href="#principal-components-of-financial-returns" title="Link to this heading">#</a></h2>
<p>So far, we have taken a look at PCA in an abstract way. However, let us approach the topic with financial data and find out, that these concepts can become more intuitive than you might think right now. In the cell below, we import daily returns (in %) from the year 2024 for Apple (AAPL), Alphabet (GOOGL), JP Morgan (JPM), Morgan Stanley (MS) and Microsoft (MSFT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="nn">yf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">tickers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;AAPL&quot;</span><span class="p">,</span> <span class="s2">&quot;MSFT&quot;</span><span class="p">,</span> <span class="s2">&quot;GOOGL&quot;</span><span class="p">,</span> <span class="s2">&quot;JPM&quot;</span><span class="p">,</span> <span class="s2">&quot;MS&quot;</span><span class="p">]</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/aapl_msft_googl_jpm_ms.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">tickers</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s2">&quot;2024-01-01&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2024-12-31&quot;</span><span class="p">)[</span><span class="s2">&quot;Adj Close&quot;</span><span class="p">]</span>
    <span class="n">data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;../data/aapl_msft_googl_jpm_ms.csv&quot;</span><span class="p">)</span>

<span class="n">r_t</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">r_t</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>GOOGL</th>
      <th>JPM</th>
      <th>MS</th>
      <th>MSFT</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2024-01-03</th>
      <td>-0.748762</td>
      <td>0.542804</td>
      <td>-0.435847</td>
      <td>-2.119281</td>
      <td>-0.072800</td>
    </tr>
    <tr>
      <th>2024-01-04</th>
      <td>-1.270009</td>
      <td>-1.821190</td>
      <td>0.663608</td>
      <td>0.261127</td>
      <td>-0.717762</td>
    </tr>
    <tr>
      <th>2024-01-05</th>
      <td>-0.401297</td>
      <td>-0.483901</td>
      <td>0.501722</td>
      <td>1.182850</td>
      <td>-0.051638</td>
    </tr>
    <tr>
      <th>2024-01-08</th>
      <td>2.417483</td>
      <td>2.291305</td>
      <td>-0.145125</td>
      <td>0.289586</td>
      <td>1.887162</td>
    </tr>
    <tr>
      <th>2024-01-09</th>
      <td>-0.226342</td>
      <td>1.519752</td>
      <td>-0.790605</td>
      <td>-1.550653</td>
      <td>0.293577</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In the next cell, we take a look at the covariance matrix. Values on the diagonal are estimated variances for each company, the values in the lower and upper triangular matrix correspond to pairwise covariances. We observe that Alphabet has the highest variance after Morgan Stanley. Taking a look at off-diagonal values, we observe pairwise positive covariance for each pair of companies. The latter is typical for stock market returns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">PcaPortfolio</span>

<span class="n">pca_portfolio</span> <span class="o">=</span> <span class="n">PcaPortfolio</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">pca_portfolio</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">pca_portfolio</span><span class="o">.</span><span class="n">plot_covariance_matrix</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eb8bc7be49617c73136154d8456ab87dc49cb4d20969c42e79275029f8e0a746.png" src="_images/eb8bc7be49617c73136154d8456ab87dc49cb4d20969c42e79275029f8e0a746.png" />
</div>
</div>
<p>The next cell exhibits the principal components. Most important are the first components as they capture the vast majority of variation in the data. In this context, variation relates to variance which is one of the most important dimensions to measure risk on financial markets. From the visualization of the previous subchapter, we know that the each component represents a vector. The vector of the first component points in the direction of the highest level of variation in the data. The higher the weight of each company, the more the vector is oriented along the variation of the individual company. If the returns of all companies would be independent, the first component would be a vector of zeros except for the company with the highest variance. Given the positive relationship for all company pairs, the companies with the highest individual variance and highest covariances get the highest weights.</p>
<p>When we take a look at the first component below, we identify Alphabet to be the company with the highest weight, followed by Morgan Stanley. Furthermore, we observe that all weights have the same sign. Two data driven insights can be drawn from this. (1) Alphabet and Morgan Stanley contribute most to the variation in the data. (2) Positive realizations from the first principal component correspond to days where the majority of all companies have a positive return, while the opposite is true if the score of the first principal component is negative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca_portfolio</span><span class="o">.</span><span class="n">components_to_df</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Component 1</th>
      <th>Component 2</th>
      <th>Component 3</th>
      <th>Component 4</th>
      <th>Component 5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AAPL</th>
      <td>0.307198</td>
      <td>-0.351721</td>
      <td>-0.808052</td>
      <td>0.176444</td>
      <td>-0.312794</td>
    </tr>
    <tr>
      <th>GOOGL</th>
      <td>0.596108</td>
      <td>-0.470881</td>
      <td>0.566225</td>
      <td>0.059440</td>
      <td>-0.314295</td>
    </tr>
    <tr>
      <th>JPM</th>
      <td>0.392371</td>
      <td>0.519710</td>
      <td>0.025992</td>
      <td>0.742899</td>
      <td>0.152879</td>
    </tr>
    <tr>
      <th>MS</th>
      <td>0.495110</td>
      <td>0.568215</td>
      <td>-0.106985</td>
      <td>-0.609917</td>
      <td>-0.220347</td>
    </tr>
    <tr>
      <th>MSFT</th>
      <td>0.388839</td>
      <td>-0.248185</td>
      <td>-0.119660</td>
      <td>-0.203559</td>
      <td>0.855250</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let us verify these statements. Assume we hold an equally weighted portfolio of all five companies over the whole time period. This means we invest 20% of our funds into every company. The daily portfolio return is given by:</p>
<div class="math notranslate nohighlight">
\[
r_{t, pf} = \sum_{j=1}^5 \frac{1}{5} r_{tj}
\]</div>
<p>with <span class="math notranslate nohighlight">\(r_{tj}\)</span> being the return of company <span class="math notranslate nohighlight">\(j\)</span> on day <span class="math notranslate nohighlight">\(t\)</span>. In the cell below, we identify the top ten days with the highest deviation from the average portfolio return. I.e., the ten days in the observation period with the highest values for <span class="math notranslate nohighlight">\(\left(r_{t, pf} - \bar{r}_{t, pf}\right)^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r_pf</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">top_variance_pf_days</span> <span class="o">=</span> <span class="n">r_pf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">r_pf</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="n">r_pf</span><span class="p">[</span><span class="n">top_variance_pf_days</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Date
2024-11-06    33.495151
2024-08-05    13.840441
2024-12-18    13.086517
2024-09-03     8.342470
2024-08-02     7.650406
2024-07-24     7.623380
2024-01-31     7.450695
2024-09-06     5.492179
2024-10-31     5.327111
2024-04-26     5.813151
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Now let us find out which companies contribute most to these deviations. Below, we examine the individual values <span class="math notranslate nohighlight">\(\left(r_{t, j} - \bar{r}_{t, j}\right)^2\)</span> at these days.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r_t_mc</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">r_t</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">r_t_mc</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">top_variance_pf_days</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>GOOGL</th>
      <th>JPM</th>
      <th>MS</th>
      <th>MSFT</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2024-11-06</th>
      <td>0.2181</td>
      <td>14.7269</td>
      <td>129.6979</td>
      <td>131.4107</td>
      <td>4.1993</td>
    </tr>
    <tr>
      <th>2024-08-05</th>
      <td>24.5725</td>
      <td>21.1329</td>
      <td>5.2221</td>
      <td>16.7456</td>
      <td>11.1271</td>
    </tr>
    <tr>
      <th>2024-12-18</th>
      <td>5.2100</td>
      <td>14.0111</td>
      <td>12.3068</td>
      <td>29.0898</td>
      <td>14.6394</td>
    </tr>
    <tr>
      <th>2024-09-03</th>
      <td>8.1847</td>
      <td>14.7114</td>
      <td>4.6562</td>
      <td>18.8118</td>
      <td>3.6709</td>
    </tr>
    <tr>
      <th>2024-08-02</th>
      <td>0.2987</td>
      <td>6.5122</td>
      <td>19.3358</td>
      <td>35.4770</td>
      <td>4.5653</td>
    </tr>
    <tr>
      <th>2024-07-24</th>
      <td>9.0951</td>
      <td>26.9326</td>
      <td>0.9669</td>
      <td>2.6463</td>
      <td>13.3629</td>
    </tr>
    <tr>
      <th>2024-01-31</th>
      <td>4.3103</td>
      <td>58.5410</td>
      <td>1.5366</td>
      <td>0.3389</td>
      <td>7.6435</td>
    </tr>
    <tr>
      <th>2024-09-06</th>
      <td>0.7087</td>
      <td>17.3907</td>
      <td>6.4092</td>
      <td>9.8069</td>
      <td>2.9179</td>
    </tr>
    <tr>
      <th>2024-10-31</th>
      <td>3.8468</td>
      <td>4.2894</td>
      <td>1.6018</td>
      <td>0.6166</td>
      <td>37.4888</td>
    </tr>
    <tr>
      <th>2024-04-26</th>
      <td>0.2378</td>
      <td>101.4753</td>
      <td>0.0088</td>
      <td>0.0205</td>
      <td>3.0776</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>To summarize the contribution over these days, we sum all squared deviations from the mean over the ten days. It turns out, the the order of risk contribution on days at which the portfolio experiences the highest variation is in line with the weights of the first principal component. Thus, these weights may be used as a identification of the highest risk contributing companies in a portfolio.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r_t_mc</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">top_variance_pf_days</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AAPL      56.6828
GOOGL    279.7236
JPM      181.7421
MS       244.9642
MSFT     102.6928
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>The first principal component is often used as a systematic risk factor for the companies in the data set. The weights all have the sign. Applying the notation for returns, the first principal component scores of the first component are derived by:</p>
<div class="math notranslate nohighlight">
\[
z_{t1} = r_{t1} w_{11} + r_{t2} w_{21} + ... + r_{t5} w_{51}
\]</div>
<p>In our example all weights are positive, this means if the majority of the five companies exhibits a positive return <span class="math notranslate nohighlight">\(z_{t1} &gt; 0\)</span> and <span class="math notranslate nohighlight">\(z_{t1} &lt; 0\)</span> on days at which the majority of the companies exhibit a negative return. The cell below exhibits the return observations for the ten days with the lowest values of the first principal component. As we can see these are days on which adverse value developments occur systematically among all companies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">pca_portfolio</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">r_t_mc</span><span class="p">)</span>
<span class="n">r_t</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Z</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">10</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>GOOGL</th>
      <th>JPM</th>
      <th>MS</th>
      <th>MSFT</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2024-08-05</th>
      <td>-4.816695</td>
      <td>-4.446178</td>
      <td>-2.129154</td>
      <td>-3.943665</td>
      <td>-3.265681</td>
    </tr>
    <tr>
      <th>2024-12-18</th>
      <td>-2.142178</td>
      <td>-3.592265</td>
      <td>-3.352075</td>
      <td>-5.245027</td>
      <td>-3.756101</td>
    </tr>
    <tr>
      <th>2024-01-31</th>
      <td>-1.935758</td>
      <td>-7.500331</td>
      <td>-1.083563</td>
      <td>-0.433694</td>
      <td>-2.694635</td>
    </tr>
    <tr>
      <th>2024-09-03</th>
      <td>-2.720527</td>
      <td>-3.684663</td>
      <td>-2.001781</td>
      <td>-4.188791</td>
      <td>-1.845906</td>
    </tr>
    <tr>
      <th>2024-08-02</th>
      <td>0.686936</td>
      <td>-2.401028</td>
      <td>-4.241208</td>
      <td>-5.807783</td>
      <td>-2.066601</td>
    </tr>
    <tr>
      <th>2024-07-24</th>
      <td>-2.875433</td>
      <td>-5.038782</td>
      <td>-0.827278</td>
      <td>-1.478259</td>
      <td>-3.585483</td>
    </tr>
    <tr>
      <th>2024-09-06</th>
      <td>-0.701496</td>
      <td>-4.019342</td>
      <td>-2.375595</td>
      <td>-2.983125</td>
      <td>-1.638141</td>
    </tr>
    <tr>
      <th>2024-10-31</th>
      <td>-1.820947</td>
      <td>-1.920209</td>
      <td>-1.109579</td>
      <td>-0.636775</td>
      <td>-6.052758</td>
    </tr>
    <tr>
      <th>2024-07-18</th>
      <td>-2.053485</td>
      <td>-1.839574</td>
      <td>-3.177020</td>
      <td>-2.010099</td>
      <td>-0.710224</td>
    </tr>
    <tr>
      <th>2024-02-13</th>
      <td>-1.127443</td>
      <td>-1.620007</td>
      <td>-0.870371</td>
      <td>-3.338315</td>
      <td>-2.152871</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The next cell exhibits returns on the ten days with the highest realizations of the first principal component. These are days with systematically positive market value development. As you can see this does not necessarily imply that all returns are positive for each company, however, it holds true for the vast majority of companies and days.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r_t</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Z</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>GOOGL</th>
      <th>JPM</th>
      <th>MS</th>
      <th>MSFT</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2024-12-16</th>
      <td>1.172768</td>
      <td>3.603412</td>
      <td>-0.150038</td>
      <td>1.279433</td>
      <td>0.965861</td>
    </tr>
    <tr>
      <th>2024-10-04</th>
      <td>0.500736</td>
      <td>0.723500</td>
      <td>3.549370</td>
      <td>3.165343</td>
      <td>-0.115238</td>
    </tr>
    <tr>
      <th>2024-07-01</th>
      <td>2.910461</td>
      <td>0.461164</td>
      <td>1.577182</td>
      <td>2.026954</td>
      <td>2.188161</td>
    </tr>
    <tr>
      <th>2024-03-20</th>
      <td>1.470929</td>
      <td>1.163030</td>
      <td>1.310692</td>
      <td>3.313723</td>
      <td>0.906484</td>
    </tr>
    <tr>
      <th>2024-01-19</th>
      <td>1.553294</td>
      <td>2.021199</td>
      <td>1.726193</td>
      <td>1.716326</td>
      <td>1.218688</td>
    </tr>
    <tr>
      <th>2024-09-19</th>
      <td>3.706554</td>
      <td>1.457981</td>
      <td>1.421484</td>
      <td>1.440075</td>
      <td>1.829114</td>
    </tr>
    <tr>
      <th>2024-12-11</th>
      <td>-0.516608</td>
      <td>5.524651</td>
      <td>0.275878</td>
      <td>0.630961</td>
      <td>1.276702</td>
    </tr>
    <tr>
      <th>2024-08-08</th>
      <td>1.663322</td>
      <td>1.944126</td>
      <td>1.826349</td>
      <td>2.735856</td>
      <td>1.069193</td>
    </tr>
    <tr>
      <th>2024-04-26</th>
      <td>-0.347285</td>
      <td>10.224373</td>
      <td>0.062062</td>
      <td>0.291718</td>
      <td>1.824370</td>
    </tr>
    <tr>
      <th>2024-11-06</th>
      <td>-0.326690</td>
      <td>3.988445</td>
      <td>11.544535</td>
      <td>11.611924</td>
      <td>2.119283</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Also the second principal component seems to be build in a way such that it captures systematic behavior. However, not in terms of systematic for all companies, but, for subgroups of all companies. If we take a look a the weights for it, we identify different signs for the companies of the technology and the banking industry. The weights are negative for the technology companies and positive for the banking companies. This means if the realization for the second principal component is negative, this is likely a day on which the technology companies outperform banks and vice versa for days with positive values of the second principal component.</p>
<p>The cell below exhibits the ten days with the lowest values of the second principal component. We observe that the returns of JP Morgan and Morgan Stanley are mostly negative and lower than the ones for the reamining companies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r_t</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Z</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">10</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>GOOGL</th>
      <th>JPM</th>
      <th>MS</th>
      <th>MSFT</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2024-04-11</th>
      <td>4.327095</td>
      <td>2.094278</td>
      <td>-0.020470</td>
      <td>-5.248227</td>
      <td>1.103332</td>
    </tr>
    <tr>
      <th>2024-11-07</th>
      <td>2.137209</td>
      <td>2.402134</td>
      <td>-4.322834</td>
      <td>-2.319570</td>
      <td>1.249465</td>
    </tr>
    <tr>
      <th>2024-06-11</th>
      <td>7.264916</td>
      <td>0.919943</td>
      <td>-2.630137</td>
      <td>-0.995953</td>
      <td>1.124170</td>
    </tr>
    <tr>
      <th>2024-04-26</th>
      <td>-0.347285</td>
      <td>10.224373</td>
      <td>0.062062</td>
      <td>0.291718</td>
      <td>1.824370</td>
    </tr>
    <tr>
      <th>2024-08-02</th>
      <td>0.686936</td>
      <td>-2.401028</td>
      <td>-4.241208</td>
      <td>-5.807783</td>
      <td>-2.066601</td>
    </tr>
    <tr>
      <th>2024-09-10</th>
      <td>-0.362137</td>
      <td>-0.033622</td>
      <td>-5.188872</td>
      <td>-1.588915</td>
      <td>2.090114</td>
    </tr>
    <tr>
      <th>2024-12-10</th>
      <td>0.413376</td>
      <td>5.588187</td>
      <td>-0.389646</td>
      <td>-1.392130</td>
      <td>-0.603113</td>
    </tr>
    <tr>
      <th>2024-07-05</th>
      <td>2.162034</td>
      <td>2.572375</td>
      <td>-1.325051</td>
      <td>-0.608786</td>
      <td>1.473624</td>
    </tr>
    <tr>
      <th>2024-04-12</th>
      <td>0.862664</td>
      <td>-1.053888</td>
      <td>-6.467784</td>
      <td>-0.748504</td>
      <td>-1.409099</td>
    </tr>
    <tr>
      <th>2024-03-14</th>
      <td>1.092732</td>
      <td>2.367853</td>
      <td>-1.781798</td>
      <td>-0.279806</td>
      <td>2.437963</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If we take a look at the ten days with the highest scores for the second principal component, we examine higher returns for JP Morgan and Morgan Stanley in comparison to the remaining companies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r_t</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Z</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>GOOGL</th>
      <th>JPM</th>
      <th>MS</th>
      <th>MSFT</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2024-07-11</th>
      <td>-2.322077</td>
      <td>-2.934396</td>
      <td>-0.168439</td>
      <td>1.140532</td>
      <td>-2.477209</td>
    </tr>
    <tr>
      <th>2024-10-11</th>
      <td>-0.650539</td>
      <td>0.715698</td>
      <td>4.439954</td>
      <td>2.202070</td>
      <td>0.115432</td>
    </tr>
    <tr>
      <th>2024-11-15</th>
      <td>-1.410920</td>
      <td>-1.759878</td>
      <td>1.422253</td>
      <td>1.230843</td>
      <td>-2.785264</td>
    </tr>
    <tr>
      <th>2024-02-15</th>
      <td>-0.157473</td>
      <td>-2.172128</td>
      <td>2.181432</td>
      <td>1.988102</td>
      <td>-0.715517</td>
    </tr>
    <tr>
      <th>2024-03-21</th>
      <td>-4.085753</td>
      <td>-0.766436</td>
      <td>1.390515</td>
      <td>2.244125</td>
      <td>0.973586</td>
    </tr>
    <tr>
      <th>2024-01-31</th>
      <td>-1.935758</td>
      <td>-7.500331</td>
      <td>-1.083563</td>
      <td>-0.433694</td>
      <td>-2.694635</td>
    </tr>
    <tr>
      <th>2024-10-16</th>
      <td>-0.885187</td>
      <td>-0.181319</td>
      <td>0.562076</td>
      <td>6.496164</td>
      <td>-0.625685</td>
    </tr>
    <tr>
      <th>2024-11-21</th>
      <td>-0.209605</td>
      <td>-4.744848</td>
      <td>1.652959</td>
      <td>2.505887</td>
      <td>-0.630580</td>
    </tr>
    <tr>
      <th>2024-03-04</th>
      <td>-2.538138</td>
      <td>-2.763596</td>
      <td>0.750175</td>
      <td>4.116553</td>
      <td>-0.139583</td>
    </tr>
    <tr>
      <th>2024-11-06</th>
      <td>-0.326690</td>
      <td>3.988445</td>
      <td>11.544535</td>
      <td>11.611924</td>
      <td>2.119283</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The cell below visualizes the cumulative variance ratio of the principal components. About 85% of all variation is captures by the first two principal component. This means for the five companies, a large level of all variation can be explained by the first systematic component and the sector affiliation of all companies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca_portfolio</span><span class="o">.</span><span class="n">plot_variance_ratio</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3b804f1d104a0e69249ca09fda1e64786d48bc3726c7ad75bf15284112e2b143.png" src="_images/3b804f1d104a0e69249ca09fda1e64786d48bc3726c7ad75bf15284112e2b143.png" />
</div>
</div>
<p>Now, let us examine the same type of analysis with a more realistic portfolio in terms of portfolio size. Below, we import data for all companies of the Euro Stoxx 50 which includes 50 large companies of the Euro zone.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="nn">yf</span>

<span class="n">eu50_tables</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_html</span><span class="p">(</span><span class="s2">&quot;https://en.wikipedia.org/wiki/EURO_STOXX_50&quot;</span><span class="p">)</span>
<span class="n">eu50_constituents</span> <span class="o">=</span> <span class="n">eu50_tables</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

<span class="n">tickers</span> <span class="o">=</span> <span class="n">eu50_constituents</span><span class="p">[</span><span class="s2">&quot;Ticker&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/stoxx_50.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">tickers</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s2">&quot;2024-01-01&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2024-12-31&quot;</span><span class="p">)[</span><span class="s2">&quot;Adj Close&quot;</span><span class="p">]</span>
    <span class="n">data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;../data/stoxx_50.csv&quot;</span><span class="p">)</span>

<span class="n">data</span><span class="o">.</span><span class="n">ffill</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">r_t</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">*</span><span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<p>Taking a look how much of the variation is captured by the principal components, we observe that only around 30% of the variation is captured by the first principal component. While this seems to be not that much, we will see below that this already captures most of the systematic movement of the companies on average.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">PcaPortfolio</span>

<span class="n">pca_portfolio</span> <span class="o">=</span> <span class="n">PcaPortfolio</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pca_portfolio</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">pca_portfolio</span><span class="o">.</span><span class="n">plot_variance_ratio</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3459f6b11f8eef4e4092144e609de52e84a61107909deb7980f43e2647af5246.png" src="_images/3459f6b11f8eef4e4092144e609de52e84a61107909deb7980f43e2647af5246.png" />
</div>
</div>
<p>To demonstrate this, we build a naive portfolio of the companies, calculate the scores for the first five principal components and determine the correlation between the scores and the portfolio returns. We can observe that the scores of the first principal component almost perfectly correlate with the portfolio returns, while even the second component scores almost do not correlate anymore. The naive portfolio return is the average market value development of all companies. According to the high correlation, the first principal component almost perfectly captures this average, systematic development.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">pca_portfolio</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">Z</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;PC_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="n">r_pf</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s2">&quot;Portfolio&quot;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">r_pf</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Portfolio</th>
      <th>PC_1</th>
      <th>PC_2</th>
      <th>PC_3</th>
      <th>PC_4</th>
      <th>PC_5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Portfolio</th>
      <td>1.000000</td>
      <td>9.944973e-01</td>
      <td>-7.032176e-02</td>
      <td>-8.499983e-03</td>
      <td>-2.502608e-02</td>
      <td>-1.384320e-02</td>
    </tr>
    <tr>
      <th>PC_1</th>
      <td>0.994497</td>
      <td>1.000000e+00</td>
      <td>2.340503e-16</td>
      <td>-1.701601e-16</td>
      <td>1.421432e-16</td>
      <td>4.345851e-16</td>
    </tr>
    <tr>
      <th>PC_2</th>
      <td>-0.070322</td>
      <td>2.340503e-16</td>
      <td>1.000000e+00</td>
      <td>2.942733e-16</td>
      <td>-2.317204e-16</td>
      <td>3.201760e-16</td>
    </tr>
    <tr>
      <th>PC_3</th>
      <td>-0.008500</td>
      <td>-1.701601e-16</td>
      <td>2.942733e-16</td>
      <td>1.000000e+00</td>
      <td>6.639647e-16</td>
      <td>-3.714232e-16</td>
    </tr>
    <tr>
      <th>PC_4</th>
      <td>-0.025026</td>
      <td>1.421432e-16</td>
      <td>-2.317204e-16</td>
      <td>6.639647e-16</td>
      <td>1.000000e+00</td>
      <td>-2.018187e-16</td>
    </tr>
    <tr>
      <th>PC_5</th>
      <td>-0.013843</td>
      <td>4.345851e-16</td>
      <td>3.201760e-16</td>
      <td>-3.714232e-16</td>
      <td>-2.018187e-16</td>
      <td>1.000000e+00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We also learned in the small company size example before that the size of the first component weights may provide some insights regarding the risk contribution of individual companies. Below, we take a look at the size of weights in the first principal component by means of a bar chart. As many companies have high and similar weight sizes, it becomes more difficult to extract a few risk driving companies. This would only be reasonable if the distribution of component weights would be less uniform and more centered towards a few companies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">pca_portfolio</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">pca_portfolio</span><span class="o">.</span><span class="n">components_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;First Principal Component Weights&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e088b7a5f9e23c4b7a60e241c34de94857f3c8f45177c4f54a62b27ed033df80.png" src="_images/e088b7a5f9e23c4b7a60e241c34de94857f3c8f45177c4f54a62b27ed033df80.png" />
</div>
</div>
<p>However, principal components, and, especially the first component are capable to capture the systematic risk in a company universe. The cell below illustrates the rolling average of the past ten observations for the naive portfolio and the first principle components. This highlights that the first component captures the current trend in the market development.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">r_pf</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subplots</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8f03f2b4cf8bd1057604b0ed37f5695037507959a2a247e1d061211712c1ac7c.png" src="_images/8f03f2b4cf8bd1057604b0ed37f5695037507959a2a247e1d061211712c1ac7c.png" />
</div>
</div>
<p>Thus, using the principal components as risk factors is a useful application. In the end of the chapter about linear regression, we already seen risk factor models where the risk factors are constructed by forming portfolios based on firm attributes. These risk factors are hard to build and only publicly available for few countries. Risk factors by PCA can be build easy and flexbile. They only need return data which usually is freely available. Below, we regress each of the Stoxx 50 companies on the first three principal components and build a histogram for the <span class="math notranslate nohighlight">\(R^2\)</span> values of these regressions.</p>
<p>As we can see, the average coefficient of determination is in the range between 40% and 50% which tells us that almost half of the variation over time of each return time series can be explained by the risk factors made of PCA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">pca_portfolio</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">r2_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">r_t</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">r_t</span><span class="p">[</span><span class="n">company</span><span class="p">])</span>
    <span class="n">r2_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">r_t</span><span class="p">[</span><span class="n">company</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;R2 Scores of Linear Regression Models with 3 Principal Components&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;R2 Score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bfc2420766dcbd0d6318621ac2136b181a2113a51467664fb353bd97e552b990.png" src="_images/bfc2420766dcbd0d6318621ac2136b181a2113a51467664fb353bd97e552b990.png" />
</div>
</div>
<p>Note, that these results might be a little too optimistic, if we use the component weights out of sample and out of time. In a more realistic setting, we first estimate the component weights with historic data and use these weights on new and unseen data in the future. Furthermore, the goal is to build universal risk factors which are also good in explaining the variation of returns for companies that are not included in the process of component estimation.</p>
<p>The out of time aspect is usually less problematic and tackled by a rolling window approach. This means at fixed periods, the weights are updated. For instance, every month, we use the last two years to estimate the weights. As long as the weights do not change drastically within a month, the risk factors with new data should lead to similar levels of explained variation.</p>
<p>The aspect of universal risk factors is usually driven by regional and sectoral considerations. Empirically, intra-continental correlations are higher than inter-continental correlations. Thus, if we want to find risk factors for European countries, it is better to choose companies from Europe to build the risk factors. Furthermore, the companies we choose should be heterogeneous.</p>
<p>As an experiment, we collect data for Dax 40 companies and regress them on the first three components of the Stoxx 50. Even though, the average level for the coefficient of determination decreases, the results are sill promising w.r.t. the variation explained.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">tickers</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_html</span><span class="p">(</span><span class="s2">&quot;https://en.wikipedia.org/wiki/DAX#Components&quot;</span><span class="p">)[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">Ticker</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/dax_40.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">tickers</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s2">&quot;2024-01-01&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2024-12-31&quot;</span><span class="p">)[</span><span class="s2">&quot;Adj Close&quot;</span><span class="p">]</span>
    <span class="n">data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;../data/dax_40.csv&quot;</span><span class="p">)</span>

<span class="n">data</span><span class="o">.</span><span class="n">ffill</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">r_dax</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">*</span><span class="mi">100</span>

<span class="n">r2_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">r_dax</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">r_dax</span><span class="o">.</span><span class="n">index</span><span class="p">],</span> <span class="n">r_dax</span><span class="p">[</span><span class="n">company</span><span class="p">])</span>
    <span class="n">r2_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">r_dax</span><span class="o">.</span><span class="n">index</span><span class="p">],</span> <span class="n">r_dax</span><span class="p">[</span><span class="n">company</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;R2 Scores of Linear Regression Models with 3 Principal Components&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;R2 Score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0a5c3231a29fdc7ecb505c0c04a03bc3a347625cf172b584eb079e0c171e9fc9.png" src="_images/0a5c3231a29fdc7ecb505c0c04a03bc3a347625cf172b584eb079e0c171e9fc9.png" />
</div>
</div>
</section>
<section id="technical-notes">
<h2>Technical notes<a class="headerlink" href="#technical-notes" title="Link to this heading">#</a></h2>
<p>These are mathematical notes which are beyond the scope of this course. However, interested readers may find them useful at some point.</p>
<section id="notation">
<h3>Notation<a class="headerlink" href="#notation" title="Link to this heading">#</a></h3>
<p>Starting point is a mean-centered matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times p}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> being the number of observations and <span class="math notranslate nohighlight">\(p\)</span> being the number of variables. Let us define a <em>projection</em> with a vector <span class="math notranslate nohighlight">\(\mathbf{w}_j \in \mathbb{R}^p\)</span> by:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{z}_j = X \mathbf{w}_j
\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{w}_j\)</span> is a unit vector which means its length is normalized to one, <span class="math notranslate nohighlight">\( || \mathbf{w}_j || = 1\)</span>. As the matrix is mean centered, the covariance matrix is given by:</p>
<div class="math notranslate nohighlight">
\[
\Sigma = \frac{1}{n} X^T X
\]</div>
<p>Furthermore, the overall variation in the data is given by:</p>
<div class="math notranslate nohighlight">
\[
TSS = \sum_{i = 1}^n \sum_{j = 1}^p x_{ij}^2
\]</div>
<p>The goal of PCA is to create a series of single projections which capture most of <span class="math notranslate nohighlight">\(TSS\)</span> in decreasing order. This means <span class="math notranslate nohighlight">\(\mathbb{z}_1\)</span> should have the highest variance, <span class="math notranslate nohighlight">\(\mathbb{z}_2\)</span> the second highest variance, etc. The projections are called principal component scores the and vectors <span class="math notranslate nohighlight">\(\mathbf{w}_j\)</span> are called principal components.</p>
<p>Let us define <span class="math notranslate nohighlight">\(SS_j\)</span> as the sum of squared principal component scores. The proportion of the variance in the data which is captured by each of the principal components is defined by:</p>
<div class="math notranslate nohighlight">
\[
PSS_j = \frac{SS_j}{TSS} = \frac{\sum_{i=1}^n z_{ij}^2}{\sum_{i = 1}^n \sum_{j = 1}^p x_{ij}^2}
\]</div>
<p>As the first principal component scores have the highest variance, <span class="math notranslate nohighlight">\(\sum_{i=1}^n z_{i1}^2\)</span> and <span class="math notranslate nohighlight">\(RSS_1\)</span> will be the largest. To determine how much of <span class="math notranslate nohighlight">\(TSS\)</span> is captured by the first <span class="math notranslate nohighlight">\(q \leq p\)</span> principal components, we can calculate the cumulative proportion of variance by:</p>
<div class="math notranslate nohighlight">
\[
CSS_q = \sum_{k=1}^q PSS_j = \frac{\sum_{i=1}^n \sum_{j=1}^q z_{ij}^2}{\sum_{i = 1}^n \sum_{j = 1}^p x_{ij}^2}
\]</div>
</section>
<section id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h3>
<p>The variance of each <span class="math notranslate nohighlight">\(\mathbb{z}_j\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\sigma_{z_j}^2 = \mathbf{w}_j^T \Sigma \mathbf{w}_j  
\]</div>
<p>with <span class="math notranslate nohighlight">\(\Sigma\)</span> being the covariance matrix of X, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\Sigma = \frac{1}{n} X^T X
\]</div>
<p>To find the first principal component, we aim to find the vector <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> that maximizes this variance:</p>
<div class="math notranslate nohighlight">
\[
\max_{\mathbf{w}_1} \mathbf{w}_1^T \Sigma \mathbf{w}_1 \quad \text{subject to } \mathbf{w}_1^T \mathbf{w}_1 = 1
\]</div>
<p>This is a constrained optimization problem. To solve it, we use the method of Lagrange multipliers. Define the Lagrangian:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mathbf{w}_1, \lambda) = \mathbf{w}_1^T \Sigma \mathbf{w}_1 - \lambda (\mathbf{w}_1^T \mathbf{w}_1 - 1)
\]</div>
<p>To find the stationary points, take the derivatives of <span class="math notranslate nohighlight">\( \mathcal{L} \)</span> with respect to  <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>, and set them to zero:</p>
<ol class="arabic">
<li><p>Derivative with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>:
$<span class="math notranslate nohighlight">\(
\frac{\partial \mathcal{L}}{\partial \mathbf{w}_1} = 2\Sigma \mathbf{w}_1 - 2\lambda \mathbf{w}_1 = 0 \quad \Rightarrow \quad \Sigma \mathbf{w}_1 = \lambda \mathbf{w}_1
\)</span>$</p>
<p>This shows that <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> must be an eigenvector of <span class="math notranslate nohighlight">\( \Sigma \)</span>, and <span class="math notranslate nohighlight">\( \lambda \)</span> is the corresponding eigenvalue, because per definition, if, for a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, a matrix <span class="math notranslate nohighlight">\(A\)</span> and a scalar <span class="math notranslate nohighlight">\(s\)</span> it holds that <span class="math notranslate nohighlight">\(A \mathbf{v} = s \mathbf{v}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is an eigenvector and <span class="math notranslate nohighlight">\(s\)</span> is its eigenvalue.</p>
</li>
<li><p>Derivative with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>:
$<span class="math notranslate nohighlight">\(
\frac{\partial \mathcal{L}}{\partial \lambda} = - (\mathbf{w}_1^T \mathbf{w}_1 - 1) = 0 \quad \Rightarrow \quad \mathbf{w}_1^T \mathbf{w}_1 = 1
\)</span>$</p></li>
</ol>
<p>For all remaining principal components, the optimization is the same, except that a further condition is orthogonality to the other principal components. For instance, to determine the second principal component, we optimize:</p>
<div class="math notranslate nohighlight">
\[
\max_{\mathbf{w}_2} \mathbf{w}_2^T \Sigma \mathbf{w}_2 \quad \text{subject to } \mathbf{w}_2^T \mathbf{w}_2 = 1 \text{ and } \mathbf{w}_2^T \mathbf{w}_1 = 0 
\]</div>
<p>This can be written as a constrained optimization problem. Using the method of Lagrange multipliers, we define the Lagrangian:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mathbf{w}_2, \lambda, \mu) = \mathbf{w}_2^T \Sigma \mathbf{w}_2 - \lambda (\mathbf{w}_2^T \mathbf{w}_2 - 1) - \mu (\mathbf{w}_2^T \mathbf{w}_1)
\]</div>
<p>To find the stationary points, take the derivatives of <span class="math notranslate nohighlight">\( \mathcal{L} \)</span> with respect to  <span class="math notranslate nohighlight">\(\mathbf{w}_2\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span>, and set them to zero:</p>
<ol class="arabic">
<li><p>Derivative with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_2\)</span>:
$<span class="math notranslate nohighlight">\(
 \frac{\partial \mathcal{L}}{\partial \mathbf{w}_2} = 2\Sigma \mathbf{w}_2 - 2\lambda \mathbf{w}_2 - \mu \mathbf{w}_1 = 0
 \)</span>$</p>
<p>Rearranging gives:</p>
<div class="math notranslate nohighlight">
\[
    \Sigma \mathbf{w}_2 = \lambda \mathbf{w}_2 + \frac{\mu}{2} \mathbf{w}_1
    \]</div>
</li>
<li><p>Derivative with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>:
$<span class="math notranslate nohighlight">\(
 \frac{\partial \mathcal{L}}{\partial \lambda} = - (\mathbf{w}_2^T \mathbf{w}_2 - 1) = 0 \quad \Rightarrow \quad \mathbf{w}_2^T \mathbf{w}_2 = 1
 \)</span>$</p></li>
<li><p>Derivative with respect to <span class="math notranslate nohighlight">\(\mu\)</span>:</p></li>
</ol>
<p>This shows that the second principal component is again an eigenvector of <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p>
</section>
<section id="relation-to-singular-value-decomposition">
<h3>Relation to singular value decomposition<a class="headerlink" href="#relation-to-singular-value-decomposition" title="Link to this heading">#</a></h3>
<p>Most packages determine principal components by the singular value decomposition of <span class="math notranslate nohighlight">\(X\)</span> instead of the eigenvectors of <span class="math notranslate nohighlight">\(\Sigma\)</span>. The singular value decomposition of <span class="math notranslate nohighlight">\(X\)</span> decomposes it to:</p>
<div class="math notranslate nohighlight">
\[
X = U \Sigma_s V^T
\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U \in \mathbb{R}^{n \times n}\)</span> containing the left singular vectors</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma_s \in \mathbb{R}^{p \times p}\)</span> is a diagonal matrix with singular values</p></li>
<li><p><span class="math notranslate nohighlight">\(V \in \mathbb{R}^{p \times p}\)</span> containing the right singular vectors</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(X^T X\)</span> is a scaled version of the covariance matrix, if we replace <span class="math notranslate nohighlight">\(X\)</span> with its singular value decomposition, we get:</p>
<div class="math notranslate nohighlight">
\[
X^T X = \left( U \Sigma_s V^T \right)^T \left( U \Sigma_s V^T \right) = V \Sigma_s^T \Sigma_s V^T
\]</div>
<p>Given a matrix <span class="math notranslate nohighlight">\(A\)</span> and a matrix <span class="math notranslate nohighlight">\(Q\)</span> whose columns include all eigenvectors of <span class="math notranslate nohighlight">\(A\)</span>, the eigenvalue decomposition of A is given by:</p>
<div class="math notranslate nohighlight">
\[
A = Q \Lambda Q^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix with the corresponding eigenvalues to the eigenvectors in Q.</p>
<p>This means the right singular values of <span class="math notranslate nohighlight">\(X\)</span> are the eigenvectors of <span class="math notranslate nohighlight">\(X^T X\)</span> which have the same direction as the eigenvectors of <span class="math notranslate nohighlight">\( \Sigma = \frac{1}{n} X^T X\)</span>. Once, they are normalized to unit norm, they are identical to the eigenvectors of the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. The eigenvalues of <span class="math notranslate nohighlight">\(\Sigma\)</span> can be determined by singular values by the relationship <span class="math notranslate nohighlight">\(\lambda = \frac{s^2}{n}\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_Clustering.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Clustering</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis">Principal Component Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-components-of-financial-returns">Principal components of financial returns</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-notes">Technical notes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">Notation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-to-singular-value-decomposition">Relation to singular value decomposition</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>