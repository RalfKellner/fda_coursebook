
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The multiple linear regression model &#8212; Financial Data Analytics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05_LinearRegression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Classification" href="06_Classification.html" />
    <link rel="prev" title="The analysis of dependent variables" href="04_SupervisedLearning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_Introduction.html">
  
  
  
  
  
  
    <p class="title logo__title">Financial Data Analytics</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_Introduction.html">
                    Financial Data Analytics
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_PythonIntroduction.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_DataAccess.html">Access to data</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_DescriptiveAnalysis.html">Descriptive analysis of data</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_SupervisedLearning.html">The analysis of dependent variables</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">The multiple linear regression model</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Classification.html">Classification</a></li>


<li class="toctree-l1"><a class="reference internal" href="07_ModellAccuracy.html">Evaluation of trained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_ModelComplexity.html">Model complexity</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Regularization.html">Regularization</a></li>

<li class="toctree-l1"><a class="reference internal" href="10_Clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_DimensionalityReduction.html">Dimensionality reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_TextAnalysis.html">Text Analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F05_LinearRegression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/05_LinearRegression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The multiple linear regression model</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-linear-regression-model">Training the linear regression model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-uncertainty">Estimation uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-quality">Model quality</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessment-of-the-independent-variables">Assessment of the independent variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">Variable selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deviations-of-the-model-assumptions">Deviations of the model assumptions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-and-cross-sectional-regressions-of-asset-returns">Time-series and cross sectional regressions of asset returns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-regressions">Time-series regressions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-sectional-regressions-of-asset-return">Cross sectional regressions of asset return</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-multiple-linear-regression-model">
<h1>The multiple linear regression model<a class="headerlink" href="#the-multiple-linear-regression-model" title="Link to this heading">#</a></h1>
<p>One of the fundamental models of data analysis for numerical variables is the linear regression model. Usually, in introductory statistics courses of an undergraduate course, the linear regression model with one independent variable is discussed. We consider the more general case of the linear regression model with multiple (for us <span class="math notranslate nohighlight">\(p\)</span> variables) independent variables:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon =  \boldsymbol{\beta}^T \boldsymbol{x} + \epsilon
\]</div>
<p>with <span class="math notranslate nohighlight">\( \boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}\)</span> and <span class="math notranslate nohighlight">\( \boldsymbol{x} = \begin{pmatrix} 1 \\ x_1 \\ \vdots \\ x_p \end{pmatrix}\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notation: In introductory statistics courses, we often distinguish between the random variable <span class="math notranslate nohighlight">\(Y\)</span> and a realization <span class="math notranslate nohighlight">\(y\)</span> by using upper and lower case. To keep the notation as streamlined as possible, we dispense with this convention and infer from the context whether we are dealing with the concept of a random variable or a realization of a variable. In addition, I more often use vector or matrix notation to keep formal representations compact. Bold lowercase letters correspond to vectors, uppercase letters correspond to matrices.</p>
</div>
<p>As the name implies, only linear relationships can be captured adequately by the linear regression model. I.e. in case of the simple linear regression the function corresponds to a straight line, with two variables to a plane and in the general case to a so-called hyperplane. It is characteristic for the linear relationship that the increase by one unit of an independent variable always leads to a constant increase of the dependent variable, regardless of which value the independent variable just takes in its value range. For example, the sales <span class="math notranslate nohighlight">\(y\)</span> of a company always increase by <span class="math notranslate nohighlight">\(\beta x \)</span>, regardless of whether <span class="math notranslate nohighlight">\(x\)</span> is a small value or a large value.</p>
<p>Nevertheless, by choosing the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> the model can be calibrated. In the lower cell we see the influence of the constant <span class="math notranslate nohighlight">\(\beta_0\)</span>, by which there is a paralell shift of the line in the simple regression model. In addition, the slope of the straight line can be manipulated by the choice of <span class="math notranslate nohighlight">\(\beta_1\)</span>. The latter is true for continuous or ordinal independent variables. If <span class="math notranslate nohighlight">\(x\)</span> is a categorical variable, the straight line is again shifted parallel up or down. The value of the <span class="math notranslate nohighlight">\(\beta_j\)</span> parameters allows conclusions about the possible influence of the respective independent variables. For example, if the value is <span class="math notranslate nohighlight">\(0\)</span>, then no influence of the respective variable can be assumed. Positive (negative) values, on the other hand, are an indication of a possible relationship.</p>
<p>The model is only complete with an assumption about the residuals <span class="math notranslate nohighlight">\(\epsilon\)</span>. Often, <span class="math notranslate nohighlight">\(\epsilon\)</span> is assumed to follow a normal distribution. Besides the statistical implications, the inclusion of <span class="math notranslate nohighlight">\(\epsilon\)</span> reveals the idea of the model how data is generated. Given, information by <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and a functional relationship between these variables <span class="math notranslate nohighlight">\(f\left(\boldsymbol{x}\right) = \boldsymbol{\beta}^T \boldsymbol{x}\)</span>, we expect a certain level for the value of <span class="math notranslate nohighlight">\(y\)</span>, i.e., <span class="math notranslate nohighlight">\(E(y | \mathbf{x}) = \boldsymbol{\beta}^T \boldsymbol{x}\)</span>. This is the deterministic part of the regression model. However, as real life observations usually do not fully a fully deterministic relationship, the deviation <span class="math notranslate nohighlight">\(\epsilon = y - \boldsymbol{\beta}^T \boldsymbol{x}\)</span> is ascribed to randomness.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">b0</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">b1</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">250</span><span class="p">)</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="n">e</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta_0$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">-</span> <span class="mf">1.</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta_0 - 1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">+</span> <span class="mf">1.</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta_0 + 1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Influence of the intercept&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta_1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">+</span> <span class="p">(</span><span class="n">b1</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta_1 - 0.5$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">+</span> <span class="p">(</span><span class="n">b1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta_1 + 0.5$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Influence of the slope&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bcc79e065a81f226378715528bde6d8203ee10611c11f52eb9e4413b975236c8.png" src="_images/bcc79e065a81f226378715528bde6d8203ee10611c11f52eb9e4413b975236c8.png" />
</div>
</div>
<p>It should be noted that the regression model can also be used to represent non-linear relationships if the variables are transformed in a non-linear way. For example, if we suspect a more quadratic relationship between the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, we can include <span class="math notranslate nohighlight">\(x^2\)</span> in the model. The effect on the functional form of the model can be observed in the next cell. This approach is the polynomial regression, where of course higher polynomials can be included as a function. Here, however, we already deviate from the traditional linear regression model. The polynomial regression is one of several alternatives to the linear regression model, which creates more flexibility by increasing the complexity of the model. However, this is also often associated with challenges that will be discussed later in the course.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">b0</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">b1</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">250</span><span class="p">)</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="n">e</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">axs</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bbcf1568310c8e4e305e9b7bfef43d012081485f4c8567163ded662da664f644.png" src="_images/bbcf1568310c8e4e305e9b7bfef43d012081485f4c8567163ded662da664f644.png" />
</div>
</div>
<section id="training-the-linear-regression-model">
<h2>Training the linear regression model<a class="headerlink" href="#training-the-linear-regression-model" title="Link to this heading">#</a></h2>
<p>In the previous section we already briefly mentioned that the values of the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> are decisive for the interpretation and analysis of possible influences of the independent variables. The important question related to this is what are reasonable values for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> given the data? Since the model is supposed to represent real life relations as well as possible, the data of a sample are usually used to estimate the model parameters in such a way that the given sample is explained as well as possible by the model. In the best case, a model is obtained in this way that works equally well for new samples. If this succeeds, one can assume that the model is generally suitable to represent the relationships for the task at hand.</p>
<p>In order to fit a model to given data as well as possible, it is attempted to set the parameters in such a way that the predictions of the model are as close as possible to the real observations. If this succeeds, it can be said that the model explains the occurrence of the data as well as possible. To learn how this process is mastered, we start with a simple (but unrealistic) example. Given an observation <span class="math notranslate nohighlight">\(y = 3\)</span> with the observation of an independent variable <span class="math notranslate nohighlight">\(x = 2\)</span>. Let the equation of the regression line be:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \beta \cdot x 
\]</div>
<p>This is a straight line through the origin of the coordinate system whose slope can be influenced by the choice of <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta = 0.5$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta = 2.0$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta = -0.5$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta = -2.0$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c9396da833c6e63b6198f9508499b8739d74430805291ca2147d81a1fcc9c646.png" src="_images/c9396da833c6e63b6198f9508499b8739d74430805291ca2147d81a1fcc9c646.png" />
</div>
</div>
<p>In order to check how close the forecast is to the actual value, it makes sense to first determine the difference <span class="math notranslate nohighlight">\(y - f(x)\)</span>. However, since one is primarily interested in how much the deviation is rather than whether one is just underestimating or overestimating the actual value, the absolute value <span class="math notranslate nohighlight">\(|y - f(x)|\)</span> or the squared deviation <span class="math notranslate nohighlight">\(\left(y - f(x)\right)^2\)</span> is rather used to quantify how far apart the realization and estimation of the model are. The quadratic deviation if often used due to its useful mathematically properties. Since the estimate of the model depends on the parameter <span class="math notranslate nohighlight">\(\beta\)</span>, we want to define the cost or loss function:</p>
<div class="math notranslate nohighlight">
\[
L\left(y, f_{\beta}(x) \right) = \left(y - f_{\beta}(x)\right)^2
\]</div>
<p>In our example we can directly insert the values:</p>
<div class="math notranslate nohighlight">
\[
L\left(y, f_{\beta}(x) \right) = \left(3 - \beta \cdot 2\right)^2
\]</div>
<p>In the graph below we see the relationship between different values for <span class="math notranslate nohighlight">\(\beta\)</span> and the loss function <span class="math notranslate nohighlight">\(L\left(y, f_{\beta}(x) \right)\)</span>. It is desirable that the value of <span class="math notranslate nohighlight">\(L\left(y, f_{\beta}(x) \right)\)</span> is as small as possible, since this results in the smallest possible deviation between the forecast of the model and the realized value. Mathematically, we are thus in an optimization, or more precisely, minimization problem, in which it is a matter of minimizing the function <span class="math notranslate nohighlight">\(L\left(y, f_{\beta}(x) \right)\)</span> by the choice of <span class="math notranslate nohighlight">\(\beta\)</span>. For our example we can solve this problem relatively easily, in which we determine possible extreme points of the function by building the first derivative and setting it to zero and in the next step check by the second derivative whether the extreme point is a minimum, maximum or a turning point.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial L}{\partial \beta} = 2 (3 - \beta \cdot 2) \cdot (-2) \stackrel{!}{=} 0 \\
-12 + 8 \cdot \beta = 0 \\
\beta = \frac{3}{2}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 L}{\partial^2 \beta} = 8 &gt; 0
\]</div>
<p>We see that the solution <span class="math notranslate nohighlight">\(\beta = \frac{3}{2}\)</span> is a minimum. The corresponding line in our example goes directly through the point <span class="math notranslate nohighlight">\((2, 3)\)</span> which gives us an exact prediction for this one observation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="n">x</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">L</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">beta</span><span class="p">:</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">beta_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">beta_range</span><span class="p">,</span> <span class="n">L</span><span class="p">(</span><span class="n">beta_range</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$L\left(y, f_{\beta}(x) \right)$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss function&quot;</span><span class="p">)</span>

<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta = 1.5$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Model prediction&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/294b58f17537ea54e5a39759f768c7e80e84f1c1ca5c48656961065474f5cf2d.png" src="_images/294b58f17537ea54e5a39759f768c7e80e84f1c1ca5c48656961065474f5cf2d.png" />
</div>
</div>
<p>In the realistic case with multiple data points, it is usually not possible to generate perfect predictions of the model, no matter how well the parameters of the model are chosen. However, the parameters of the model can be determined using the same logic as in the previous example. Let us add another data point <span class="math notranslate nohighlight">\((x_2 = 3, y_2 = 2)\)</span> to our sample next to the point <span class="math notranslate nohighlight">\((x_1 = 2, y_1 = 3)\)</span>. The loss function becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
L\left(y, f_{\beta}(\boldsymbol{x}) \right) &amp; = \frac{1}{2} \left( \left(y_1 - f_{\beta}(x_1)\right)^2 +  \left(y_2 - f_{\beta}(x_2)\right)^2 \right) = \\
&amp; = \frac{1}{2} \sum_{i = 1}^2 \left(y_i - f_{\beta}(x_i)\right)^2 = \\
&amp; = \frac{1}{2} \sum_{i = 1}^2 \left(y_i - \beta x_i\right)^2 
\end{split}
\end{split}\]</div>
<p>Even if the representation becomes somewhat more formal by the sum sign, a possible minimum can be determined as before by the first derivative:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial L}{\partial \beta} = \frac{1}{2} \sum_{i=1}^2 2 \left(y_i - \beta x_i\right) \cdot (-x_i) = \sum_{i=1}^2 \left(y_i - \beta x_i\right) \cdot (-x_i) \stackrel{!}{=} 0 \\
\sum_{i=1}^2 - x_i y_i + \sum_{i=1}^2 \beta x_i^2 = 0 \\
\beta \sum_{i=1}^2 x_i^2 = \sum_{i=1}^2 x_i y_i  \\
\beta = \frac{\sum_{i=1}^2 x_i y_i }{\sum_{i=1}^2 x_i^2 }
\end{aligned}
\end{split}\]</div>
<p>In our example, this results in the value <span class="math notranslate nohighlight">\(\beta = \frac{12}{13}\)</span>. The lower graph visualizes that in this way a straight line results, which runs between the two points. The model tries to be as close as possible to both points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>

<span class="n">L</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">beta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">beta_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">L</span><span class="p">(</span><span class="n">beta_</span><span class="p">)</span> <span class="k">for</span> <span class="n">beta_</span> <span class="ow">in</span> <span class="n">beta_range</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">beta_range</span><span class="p">,</span> <span class="n">losses</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$L\left(y, f_{\beta}(x) \right)$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss function&quot;</span><span class="p">)</span>

<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mi">12</span><span class="o">/</span><span class="mi">13</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\beta = 1.5$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Model prediction&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/93d4a9f1fb338bff64680c539736fd734ab15a192160e2069ff79724d71a33dc.png" src="_images/93d4a9f1fb338bff64680c539736fd734ab15a192160e2069ff79724d71a33dc.png" />
</div>
</div>
<p>We note that the fitting of the model of the data is done by mathematical optimization. Here it is important to understand that the data points of the sample <span class="math notranslate nohighlight">\((x_1, y_1), ..., (x_n, y_n)\)</span> are invariant values and the fitting of the model is done by choosing the parameters. In the general case with multiple data points and a regression problem, the common loss function is:</p>
<div class="math notranslate nohighlight">
\[
L\left(\boldsymbol{y}, f_{\boldsymbol{\beta}}(\boldsymbol{X}) \right) = \frac{1}{n} \sum_{i=1}^n \left(y_i - \boldsymbol{\beta}^T \boldsymbol{x}_i \right)^2
\]</div>
<p>or since for the identification of the minimum the constant <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span> is insignificant, sometimes also:</p>
<div class="math notranslate nohighlight">
\[
L\left(\boldsymbol{y}, f_{\boldsymbol{\beta}}(\boldsymbol{X}) \right) = \sum_{i=1}^n \left(y_i - \boldsymbol{\beta}^T \boldsymbol{x}_i \right)^2
\]</div>
<p>Even if the minimization for the general case becomes somewhat more difficult than in our simple examples, all values for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> can be determined in an analytical way for the linear regression model. This solution is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} = \left( X^T X \right)^{-1} X^T \boldsymbol{y}
\]</div>
<p>with <span class="math notranslate nohighlight">\(X\)</span> representing the matrix which contains realizations for each variable and observations and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> including all realizations of the target variable. Note, that estimated parameters are usually given an umbrella symbol such as <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> to underline that they are estimates from the sample and not the true values of the whole population.</p>
</section>
<section id="estimation-uncertainty">
<h2>Estimation uncertainty<a class="headerlink" href="#estimation-uncertainty" title="Link to this heading">#</a></h2>
<p>Before we go into a specific example, let’s look a little more closely at the aspect of estimation uncertainty. In some cases, it may be theoretically possible to have access to the data of the entire population and thus determine the true value of a parameter. For example, it is theoretically possible to determine the average height of all adult citizens in a country. However, this often fails due to practical aspects of data collection and questions of economic benefit, as the collection of all data often involves high costs. Accordingly, as soon as one does not resort to all values of the population, but tries to determine the unknown value of the population through a random sample, the uncertainty in the estimated value arises due to the randomness of the sample collection. If a random sample is drawn more often, the random realizations and thus the estimated parameters per sample will differ. If we look at the highly simplified example in the bottom cell, we see that when random draws are made from a population and the expected value is estimated by the arithmetic mean, there will be varying values that always deviate from the true value. Such behavior always exists when the parameters of models are estimated from samples. The more the estimated parameters vary, the higher the statistical uncertainty and the associated inference. How much the parameters vary can be quantified by the standard errors. These are used to calculate important statistical ratios such as the confidence interval and the p-value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">population</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">22</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">22</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The population values are:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">population</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tri</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>

<span class="n">sample_means</span> <span class="o">=</span> <span class="n">population</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">population</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The true mean of the population is:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">true_mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample means with leave one out drawing are:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_means</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The population values are:
[22 21 25 27 22]
 
The true mean of the population is:
23.4
 
Sample means with leave one out drawing are:
[23.75 24.   23.   22.5  23.75]
</pre></div>
</div>
</div>
</div>
<p>Frequentist statistical hypothesis tests attempt to incorporate the uncertainty of the parameter estimate when testing a hypothesis. For example, in the null hypothesis, if one assumes that the variable <span class="math notranslate nohighlight">\(x_1\)</span> has no non-zero influence on the dependent variable, then due to the randomness of sampling, it is still possible that <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> takes on a non-zero value, even if the true value is <span class="math notranslate nohighlight">\(\beta = 0\)</span>. However, if one can determine the standard error of the parameter estimator, one is able to set limits above which the estimated random value of a sample for a given hypothesis seems very implausible. In addition to classical tests, confidence intervals are often included with estimated parameters. The higher the standard error of an estimator, the wider the interval, the less concrete the inference about the estimated influence of the associated variable. The exact functioning of hypothesis tests and the handling of confidence intervals should be content of introductory statistics courses of your studies, but with this discussion I would like to sensitize you to the importance of the statistical uncertainty of estimated parameters of a model. When analyzing a model, the focus should not only be on the estimated value, but also on its uncertainty.</p>
<p>To understand, how a model works, it is often helpful to create synthetic data by simulation. Very likely, this is new for you, so for the moment let us not go into the details how random numbers can be generated. Nevertheless, you should remember from statisics, that random variables are defined by their distribution. For instance a standard normal distribution’s density function looks like the one below. A density ditribution of a continuous variable only must fulfill that <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> and that <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} f(x)dx = 1\)</span>. The probability for a realization to fall in an interval <span class="math notranslate nohighlight">\([x_l, x_u]\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
P\left(x_u \leq x \leq x_u\right) = \int_{x_l}^{x_u} f(x) dx
\]</div>
<p>This means the larger the area under the density curve over an interval, the higher the probability that a random realizaton falls within the interal. If we interpret probabilities as empirical frequencies, this means relatively often, we are going to observe realizations in intervals with large areas under the density function. For instance, the frequency of realizations in the interval <span class="math notranslate nohighlight">\([-1, 1]\)</span> are going to be higher than in the inveral <span class="math notranslate nohighlight">\([-4, -3]\)</span> for a standard normal distribution. If we simulate data from a standard normal distribution, frequencies of realizations in certain intervals will be close but not exactly equal to the corresponding probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Standard Normal Distribution&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standard Normal Distribution (Mean = 0, Std Dev = 1)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability Density&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_rnd</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">density</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Histogram for simulated data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequencies&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Display the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/16b63518c6b43444c95d2cd679a3e9963627f29c42e32f001b0508bc603388a5.png" src="_images/16b63518c6b43444c95d2cd679a3e9963627f29c42e32f001b0508bc603388a5.png" />
</div>
</div>
<p>To simulate data according to a regression model, we need some fixed observations for the independent variables. We simulate them below, however, keep them fixed to replicate the real life behavior that originally all values for <span class="math notranslate nohighlight">\(X\)</span> are assumed to be fixed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Number of observations</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">3</span>   <span class="c1"># Number of independent variables (excluding the intercept)</span>

<span class="n">beta_0</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Intercept</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># Coefficients for the features</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>  <span class="c1"># Normally distributed features</span>
<span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">X</span><span class="p">]</span>

<span class="n">true_standard_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_with_intercept</span><span class="p">)))</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;x_1&quot;</span><span class="p">,</span> <span class="s2">&quot;x_2&quot;</span><span class="p">,</span> <span class="s2">&quot;x_3&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x_1</th>
      <th>x_2</th>
      <th>x_3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.496714</td>
      <td>-0.138264</td>
      <td>0.647689</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.523030</td>
      <td>-0.234153</td>
      <td>-0.234137</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.579213</td>
      <td>0.767435</td>
      <td>-0.469474</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.542560</td>
      <td>-0.463418</td>
      <td>-0.465730</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.241962</td>
      <td>-1.913280</td>
      <td>-1.724918</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>To simulate data for the regression model, we further need to declare true values for the model parameters. In case of the regression model, these are the beta values and the standard deviation for <span class="math notranslate nohighlight">\(\epsilon\)</span>. In our example, we set:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{\beta} = 
\begin{pmatrix}
5 \\
4 \\
-3 \\
0
\end{pmatrix}, \sigma_{\epsilon} = 1
\end{split}\]</div>
<p>For each observation, we calculate <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^T \boldsymbol{x}\)</span>, draw a random standard normally distributed random number and add both values to simulate an obsevation for <span class="math notranslate nohighlight">\(y\)</span>. The cell below simulates data like this and creates a pairplot. The first line exhibits the pairwise realtion between the target variable <span class="math notranslate nohighlight">\(y\)</span> and the independent variables <span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span>. Note, that data changes every time you click the play button next to the cell as data is simulation is refreshed. Also take a look and execute the next cell as well. It estimates the parameters of the model for the simulated data. It is important to understand that these estimates differ from the true values (which in reality we never know). The reason for that is the random part of the data. As the random part is different for every data sample (because it is random), parameter estimates for every data sample will be different.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># Normally distributed random noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b7783070a3f101556d97d8c62341225ef7e2f1f619ef9010fc04de1d87a51b69.png" src="_images/b7783070a3f101556d97d8c62341225ef7e2f1f619ef9010fc04de1d87a51b69.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="n">X_reg</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="s2">&quot;X3&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;y&quot;</span><span class="p">]</span>
<span class="n">X_reg</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_reg</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_reg</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.961
Model:                            OLS   Adj. R-squared:                  0.960
Method:                 Least Squares   F-statistic:                     8088.
Date:                Tue, 28 Jan 2025   Prob (F-statistic):               0.00
Time:                        13:53:26   Log-Likelihood:                -1444.0
No. Observations:                1000   AIC:                             2896.
Df Residuals:                     996   BIC:                             2916.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          4.9820      0.033    152.904      0.000       4.918       5.046
X1             4.0170      0.034    119.738      0.000       3.951       4.083
X2            -3.0266      0.032    -93.747      0.000      -3.090      -2.963
X3            -0.0381      0.033     -1.149      0.251      -0.103       0.027
==============================================================================
Omnibus:                        2.961   Durbin-Watson:                   2.045
Prob(Omnibus):                  0.228   Jarque-Bera (JB):                2.542
Skew:                          -0.010   Prob(JB):                        0.281
Kurtosis:                       2.754   Cond. No.                         1.09
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>To demonstrate the variability of estimated regression coefficients (parameters), we repeatedly, create new randomly simulated datasets, estimate the parameters of the model and store their values. If we do this for 1,000 times below, we can plot the histograms of parameter estimates for the beta parameters. As you can see, parameter estimates are often close to the true values from the simulation, but, for a few sample, can also deviate to a large extent. For instance, even though, there is no relation between <span class="math notranslate nohighlight">\(x_3\)</span> and <span class="math notranslate nohighlight">\(y\)</span> because <span class="math notranslate nohighlight">\(\beta_3 = 0\)</span>, a view samples create estimates, e.g., <span class="math notranslate nohighlight">\(\hat{\beta}_3 \leq -0.05\)</span> or <span class="math notranslate nohighlight">\(\hat{\beta}_3 \geq 0.05\)</span> which might indicate a moderate impact between <span class="math notranslate nohighlight">\(x_3\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The standard deviation of these estimates are denoted as the standard error in this context. Let us denote it by <span class="math notranslate nohighlight">\(\sigma_{err}\)</span> If you take a look in the cell above, you can see the column “std err” right next to the column “coef”, the “t” column are <span class="math notranslate nohighlight">\(t\)</span> statistics under the assumption:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{H}_0:  \beta_j = 0 \\
\text{H}_1:  \beta_j \neq 0 \\
\end{split}\]</div>
<p>This is the default hypothesis for every <span class="math notranslate nohighlight">\(\beta\)</span> estimate in a regression output as shown above. If the hypothesis can be rejected, we infer that the corresponding variable has an effect which is statistically different from zero. In general, the <span class="math notranslate nohighlight">\(t\)</span> value is calculated by:</p>
<div class="math notranslate nohighlight">
\[
t = \frac{\hat{\beta} - \beta^{H_0}}{\sigma_{err}}
\]</div>
<p>However, as <span class="math notranslate nohighlight">\(\beta^{H_0} = 0\)</span> by default, for the regression output it becomes:</p>
<div class="math notranslate nohighlight">
\[
t = \frac{\hat{\beta}}{\sigma_{err}}
\]</div>
<p>The p-value next to the t statistic is calculated as: <span class="math notranslate nohighlight">\(P\left(T \geq |t_j| \right) \cdot 2\)</span></p>
<p>Thus, we may interpret the t statistic as a normalized version of the estimated parameter, the higher it is, the less the estimated parameter is in line with the null hypthesis. Very commonly, the standard null hypothesis in regression outputs is the parameter to be equal zero. If the null is rejected, we assume that <span class="math notranslate nohighlight">\(x_j\)</span> has an impact on <span class="math notranslate nohighlight">\(y\)</span> which is statistically significant different from zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">betas</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;beta_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># Normally distributed random noise</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">noise</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">])</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># No additional intercept since we include it in X</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Get the estimated coefficients</span>
    <span class="n">estimated_betas</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
    
    <span class="c1"># Store the coefficients in the results DataFrame</span>
    <span class="n">betas</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimated_betas</span>

<span class="n">betas</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2a76c2fb3a5ce24a43bab5f1ac2ef00a5e39ab7d0e08d7ef81bcaf7c16cb5db9.png" src="_images/2a76c2fb3a5ce24a43bab5f1ac2ef00a5e39ab7d0e08d7ef81bcaf7c16cb5db9.png" />
</div>
</div>
<p>Finally, let us take a look at a real example where we want to analyze a possible influence of the advertising channel on sales. In the associated output, we see that a positive influence of television and rate advertising on sales is measurable. The sign of the news variable is negative, but we can see from the p-value that we cannot assume a zero different influence of this variable. This means that, at least from a statistical point of view, it does not make sense to interpret the negative sign as a negative influence of the variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">advertising_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/Advertising.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span> <span class="s2">&quot;Unnamed: 0&quot;</span><span class="p">)</span>
<span class="n">advertising_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.897
Model:                            OLS   Adj. R-squared:                  0.896
Method:                 Least Squares   F-statistic:                     570.3
Date:                Tue, 28 Jan 2025   Prob (F-statistic):           1.58e-96
Time:                        13:53:27   Log-Likelihood:                -386.18
No. Observations:                 200   AIC:                             780.4
Df Residuals:                     196   BIC:                             793.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          2.9389      0.312      9.422      0.000       2.324       3.554
TV             0.0458      0.001     32.809      0.000       0.043       0.049
radio          0.1885      0.009     21.893      0.000       0.172       0.206
newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011
==============================================================================
Omnibus:                       60.414   Durbin-Watson:                   2.084
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241
Skew:                          -1.327   Prob(JB):                     1.44e-33
Kurtosis:                       6.332   Cond. No.                         454.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-quality">
<h2>Model quality<a class="headerlink" href="#model-quality" title="Link to this heading">#</a></h2>
<p>How good the model is at explaining the independent variable is best told by relative comparison. Good regression models should be able to make better predictions using the information from the independent variable than a model that cannot use the information from the independent variable. For example, a naive prediction and appropriate benchmark for a regression problem would be the arithmetic mean of the realizations of the dependent variable <span class="math notranslate nohighlight">\(\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i\)</span>. The better a model, the smaller should be, on average, the squared or the absolute deviations between realizations and forecasts. The forecasts of the linear regression model are given by the estimated regression function <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}^T \boldsymbol{x}\)</span>:</p>
<p>We define the mean-squared-error (MSE):</p>
<div class="math notranslate nohighlight">
\[
MSE(\boldsymbol{y}, \boldsymbol{{\hat{y}}}) = \frac{1}{n} \sum_{i = 1}^n \left(y_i - \hat{y}_i\right)^2
\]</div>
<p>As an alternative, we define the mean-absolute-error (MAE):</p>
<div class="math notranslate nohighlight">
\[
MAE(\boldsymbol{y}, \boldsymbol{{\hat{y}}}) = \frac{1}{n} \sum_{i = 1}^n |y_i - \hat{y}_i|
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}\)</span> represents the predicted value that can be generated by a different models. In comparison, the MAE is less affected by isolated high error forecasts. In both cases, the ratio from the respective metric for the regression model and for the unconditional forecast (e.g., <span class="math notranslate nohighlight">\(\bar{y}\)</span>) can be considered. For example, let us determine:</p>
<div class="math notranslate nohighlight">
\[
\frac{\sum_{i = 1}^n \left(y_i - \hat{y}_i\right)^2}{\sum_{i = 1}^n \left(y_i - \bar{y}\right)^2} 
\]</div>
<p>the smaller the value, the more advantageous the regression model. This means that with the model and its use of the independent variables, the realizations can be better predicted (and thus better explained) than without the information of the independent variables. Of course, two models can also be compared in this way, each using the information of the independent variable. Merely looking at the MSE and MAE alone is usually not very informative, since all we know is that a value close to zero is a good sign. However, as for the estimation of zero different values, it strongly depends on the numerical range of the dependent variable, which can be considered as a lower value. Therefore, it is better to always include a meaningful benchmark when evaluating the model. For the linear regression model, a normalized variant of the goodness-of-fit measure is often used, the coefficient of determination <span class="math notranslate nohighlight">\(R^2\)</span>, which is given by:</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\sum_{i = 1}^n \left(y_i - \hat{y}_i\right)^2}{\sum_{i = 1}^n \left(y_i - \bar{y}\right)^2} 
\]</div>
<p>The range of values lies between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, whereby higher values indicate a better explanatory quality of the linear regression model. If the assumption of a linear relationship between the independent and the dependent variable is violated, however, values smaller than <span class="math notranslate nohighlight">\(0\)</span> can also result. Since the model quality usually increases with the inclusion of further independent variables, the adjusted coefficient of determination should rather be used for the output of the linear regression, which corrects this property by:</p>
<div class="math notranslate nohighlight">
\[
R_{\text{adj}}^2 = 1 - \left( \frac{(1 - R^2) \cdot (n - 1)}{n - p} \right)
\]</div>
<p>In addition to quantitative model quality, it is often useful to generate a scatterplot where realizations are plotted on the x-axis and predictions are plotted on the y-axis. A perfect model would produce points along a diagonal, points above the diagonal representing overestimates and points below representing underestimates of the model. Another informative scatterplot is the visualizations of the variances <span class="math notranslate nohighlight">\(\epsilon = y - \hat{y}\)</span> across all observations. This plot can be used to visually check whether there are systematic deviations across the observations.</p>
<p>In the bottom cell, we determine the ratio of the MSE for the linear regression model and <span class="math notranslate nohighlight">\(\bar{y}\)</span>. In addition, we look at the two graphs mentioned above. Overall, the model seems to explain the sales values relatively well and, more importantly, much better than the unconditional forecast <span class="math notranslate nohighlight">\(\bar{y}\)</span>. However, it seems that the model provides better forecasts for higher sales values than for smaller values and, in particular, sales values in the middle range are systematically overestimated. These findings suggest that the linear relationship between the independent variables and the dependent variable may not fully reflect reality and that a model that is able to represent non-linear relationships may be more appropriate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">y_hat</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">mse_model</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="n">mse_benchmark</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()]</span><span class="o">*</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The mean squared error ratio of the linear regression model and the arithmetic mean of sales is: </span><span class="si">{</span><span class="n">mse_model</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mse_benchmark</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">transform</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">advertising_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;observation number&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\epsilon$&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Estimation performance of the linear regression model&quot;</span> <span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean squared error ratio of the linear regression model and the arithmetic mean of sales is: 0.1028
</pre></div>
</div>
<img alt="_images/ef2a9831ad40b5520c373d48e9bfe1d4232a12044d7d58650262fa7f0a769acb.png" src="_images/ef2a9831ad40b5520c373d48e9bfe1d4232a12044d7d58650262fa7f0a769acb.png" />
</div>
</div>
</section>
<section id="assessment-of-the-independent-variables">
<h2>Assessment of the independent variables<a class="headerlink" href="#assessment-of-the-independent-variables" title="Link to this heading">#</a></h2>
<p>Once the model has been estimated and its goodness of fit has been found to be sufficient, the results of the parameter estimation can be used to identify the variables with the greatest influence on the dependent variable. The estimated parameters of the advertising regression model are <span class="math notranslate nohighlight">\(\hat{\beta}_{\text{TV}} = 0.0458\)</span>, <span class="math notranslate nohighlight">\(\hat{\beta}_{\text{radio}} = 0.1885\)</span>, <span class="math notranslate nohighlight">\(\hat{\beta}_{\text{newspaper}} = -0.0100\)</span>. Based on these values, one would initially assume that the greatest influence comes from radio advertising, since an increase in this by one unit is accompanied by the greatest increase in the dependent variable. However, it should be noted that the numerical range of realizations of this variable may differ from those of the others, making the one-unit changes not comparable. If we look at the numerical range of the independent variables in the lower cell, this is exactly the case. In order to truly compare the estimated parameters and their impact on the dependent variable, they must first be brought to comparable ranges of values. If the model is estimated with these standardized variables, the respective influence of the independent variable can be compared on the basis of the estimated parameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>200.000000</td>
      <td>200.000000</td>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>147.042500</td>
      <td>23.264000</td>
      <td>30.554000</td>
    </tr>
    <tr>
      <th>std</th>
      <td>85.854236</td>
      <td>14.846809</td>
      <td>21.778621</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.700000</td>
      <td>0.000000</td>
      <td>0.300000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>74.375000</td>
      <td>9.975000</td>
      <td>12.750000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>149.750000</td>
      <td>22.900000</td>
      <td>25.750000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>218.825000</td>
      <td>36.525000</td>
      <td>45.100000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>296.400000</td>
      <td>49.600000</td>
      <td>114.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In the bottom cell, we estimate the model again using standardized values of the independent variables. The estimated parameters are <span class="math notranslate nohighlight">\(\hat{\beta}_{\text{TV}} = 3.9193\)</span>, <span class="math notranslate nohighlight">\(\hat{\beta}_{\text{radio}} = 2.7921\)</span>, <span class="math notranslate nohighlight">\(\hat{\beta}_{\text{newspaper}} = -0.0225\)</span>. Using these values, we can identify the largest impact due to TV advertising, which brings a <span class="math notranslate nohighlight">\(3.9193\)</span> increase in sales when TV advertising spending is increased by one standard deviation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">]]</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_scaled</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.897
Model:                            OLS   Adj. R-squared:                  0.896
Method:                 Least Squares   F-statistic:                     570.3
Date:                Tue, 28 Jan 2025   Prob (F-statistic):           1.58e-96
Time:                        13:53:27   Log-Likelihood:                -386.18
No. Observations:                 200   AIC:                             780.4
Df Residuals:                     196   BIC:                             793.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         14.0225      0.119    117.655      0.000      13.787      14.258
TV             3.9193      0.119     32.809      0.000       3.684       4.155
radio          2.7921      0.128     21.893      0.000       2.541       3.044
newspaper     -0.0225      0.128     -0.177      0.860      -0.274       0.229
==============================================================================
Omnibus:                       60.414   Durbin-Watson:                   2.084
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241
Skew:                          -1.327   Prob(JB):                     1.44e-33
Kurtosis:                       6.332   Cond. No.                         1.46
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="variable-selection">
<h2>Variable selection<a class="headerlink" href="#variable-selection" title="Link to this heading">#</a></h2>
<p>So far, we have assumed that all available variables are included in the model. However, this may not always be advantageous. In particular, if individual independent variables do not contribute positively to the improvement of the model, it makes little sense to include these variables in the model. In addition, problems can also arise if independent variables are highly correlated (collinearity). So which selection is the best for the model. With a small number of independent variables, models can theoretically be estimated for all variable combinations and compared based on their goodness of fit. However, for <span class="math notranslate nohighlight">\(p\)</span> independent variables <span class="math notranslate nohighlight">\(2^p\)</span> combination possibilities exist, so one quickly reaches the limits of computational implementation with this approach. In practice, models with a subset of all independent variables are usually determined sequentially. This can be done either in “forward” or “backward” manner. In forward selection, one starts with a model without independent variables and estimates models with one variable each. The variable that improves the quality of the model the most is included in the model first. Subsequently, models are estimated and evaluated again, each time with the addition of a variable other than the one already selected. The variable that brings the greatest improvement is again included. The process is terminated, if by the renewed addition of a variable no more significant improvement develops. What is considered a significant improvement is determined by the user. In backward selection, a model with all variables is estimated in the first step. Subsequently, models are estimated with the respective omission of a variable. The variable whose omission reduces the model quality the least is removed from the model. This process is repeated until the reduction in model goodness is deemed too high.</p>
<p>As an example, we consider the forward selection for the Advertising dataset. We use the <span class="math notranslate nohighlight">\(R^2\)</span> as a metric to quantify the goodness. In the first step, we select the TV variable. Next, we see an increase in <span class="math notranslate nohighlight">\(R^2\)</span> of <span class="math notranslate nohighlight">\(0.2853\)</span> when the radio variable is included in the model. Including the newspaper variable would not increase the <span class="math notranslate nohighlight">\(R^2\)</span> any further, so there is no need to include this variable in the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">]</span>

<span class="n">r2_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="n">variable</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">r2_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">),</span> <span class="mi">4</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variable names: </span><span class="si">{</span><span class="n">variables</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R^2 values for univariate regressions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variable with the highest R^2 value: </span><span class="si">{</span><span class="n">variables</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">)]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Continue with the selection of the next variable:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;radio&quot;</span><span class="p">,</span> <span class="s2">&quot;newspaper&quot;</span><span class="p">]</span>

<span class="n">r2_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="n">variable</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">r2_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">),</span> <span class="mi">4</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Remaining variable names: </span><span class="si">{</span><span class="n">variables</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R^2 values for regressions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variable with the highest R^2 improvement: </span><span class="si">{</span><span class="n">variables</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">)]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Variable names: [&#39;TV&#39;, &#39;radio&#39;, &#39;newspaper&#39;]
R^2 values for univariate regressions:
[0.6119, 0.332, 0.0521]
Variable with the highest R^2 value: TV
Continue with the selection of the next variable:

Remaining variable names: [&#39;radio&#39;, &#39;newspaper&#39;]
R^2 values for regressions:
[0.8972, 0.6458]
Variable with the highest R^2 improvement: radio
</pre></div>
</div>
</div>
</div>
</section>
<section id="deviations-of-the-model-assumptions">
<h2>Deviations of the model assumptions<a class="headerlink" href="#deviations-of-the-model-assumptions" title="Link to this heading">#</a></h2>
<p>The linear regression model uses simplifying assumptions. We want to discuss these again in conclusion, in order to sensitize with it, whereby wrong estimations can originate under the use of the linear regression model. Specifically, the assumptions are as follows:</p>
<ul class="simple">
<li><p>Linear relationship between the independent and the dependent variable.</p></li>
<li><p>Normal distribution of the dependent variable</p></li>
<li><p>Homoscedasticity</p></li>
<li><p>Independent error terms</p></li>
<li><p>Low correlation of the independent variables (low or no multicollinearity).</p></li>
</ul>
<p><strong>Linear relationship</strong></p>
<p>The linear regression line establishes a linear relationship between the independent and dependent variable. This means that the change of an independent variable by one unit always leads to a constant change of the dependent variable. This is not always realistic. For example, one may assume that the increase for advertising expenses above a certain amount will lose the additional benefit of increased sales. In this case, it would not be the case that the increase in advertising expenditure always leads to the same increase in sales, but rather that the increase in sales depends on how high the expenditure on advertising measures already is. Another assumption of the classical linear regression model is that the additivity of the independent variables. This means that the influence of each independent variable is independent of the other variables. This changes if, for example, interaction effects are included. With an interaction effect one is interested in interactions of the influence of several variables. For the example of the advertisement data set, we have seen after variable selection that:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_{\text{TV}} + \beta_2 x_{\text{radio}} + \epsilon
\]</div>
<p>is a good model. If we include the interaction term, the model changes to:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_{\text{TV}} + \beta_2 x_{\text{radio}} + \beta_3 x_{\text{TV}} x_{\text{radio}} + \epsilon
\]</div>
<p>Alternatively, we can make this model either to:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \left(\beta_1 + \beta_3 x_{\text{radio}} \right) x_{\text{TV}} + \beta_2 x_{\text{radio}} + \epsilon
\]</div>
<p>or to:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_{\text{TV}} + \left(\beta_2 + \beta_3 x_{\text{TV}} \right) x_{\text{radio}} + \epsilon
\]</div>
<p>Depending on the variant of the model, it becomes apparent that the influence of TV advertising or radio advertising depends on the other type of advertising, and thus the original characteristic of additivity disappears. In the bottom cell, we estimate the model with the interaction term. We see from the output that the interaction term has a significantly non-zero positive impact. In addition, the coefficient of determination improves. The positive value can be interpreted that the positive influence of one advertising measure increases when the other advertising measure is increased.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;TV&quot;</span><span class="p">,</span> <span class="s2">&quot;radio&quot;</span><span class="p">]]</span>
<span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;TV_radio&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">TV</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">radio</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">advertising_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.968
Model:                            OLS   Adj. R-squared:                  0.967
Method:                 Least Squares   F-statistic:                     1963.
Date:                Tue, 28 Jan 2025   Prob (F-statistic):          6.68e-146
Time:                        13:53:27   Log-Likelihood:                -270.14
No. Observations:                 200   AIC:                             548.3
Df Residuals:                     196   BIC:                             561.5
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          6.7502      0.248     27.233      0.000       6.261       7.239
TV             0.0191      0.002     12.699      0.000       0.016       0.022
radio          0.0289      0.009      3.241      0.001       0.011       0.046
TV_radio       0.0011   5.24e-05     20.727      0.000       0.001       0.001
==============================================================================
Omnibus:                      128.132   Durbin-Watson:                   2.224
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1183.719
Skew:                          -2.323   Prob(JB):                    9.09e-258
Kurtosis:                      13.975   Cond. No.                     1.80e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.8e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>In addition to including interactions (and thus relaxing the additivity assumption), the linear regression model can also be fitted to represent possible non-linear relationships. Possible solutions are given by adding polynomials or by splines. However, the modeling of non-linear relationships is done in a later chapter.</p>
<p><strong>Normal distribution assumption</strong></p>
<p>So far we did not discuss the difference of the regression model:</p>
<div class="math notranslate nohighlight">
\[
y = \boldsymbol{\beta}^T \boldsymbol{x} + \epsilon
\]</div>
<p>and the regression line:</p>
<div class="math notranslate nohighlight">
\[
y = \boldsymbol{\beta}^T \boldsymbol{x} 
\]</div>
<p>any further. By the regression line only the conditional expected value for <span class="math notranslate nohighlight">\(y\)</span> is determined. Conditional on the information of the independent variables <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, what value do we expect for <span class="math notranslate nohighlight">\(y\)</span>. However, in the traditional regression model, the assumption is often still made that <span class="math notranslate nohighlight">\(\epsilon\)</span> is normally distributed <span class="math notranslate nohighlight">\(\epsilon \sim N(0; \sigma^2)\)</span>. It follows that <span class="math notranslate nohighlight">\(y\)</span> is also normally distributed, <span class="math notranslate nohighlight">\(y \sim N(\boldsymbol{\beta}^T \boldsymbol{x} ; \sigma^2)\)</span>. Thus, through the model, not only can the expected value be estimated, but we can also perform calculations for other parts of the distribution. For example, given values of the independent variables, we can determine the probability of falling below or exceeding a given value for <span class="math notranslate nohighlight">\(y\)</span>. These estimates are only accurate if the normal distribution assumption is not violated. For this purpose, the Jarque-Bera test with the null hypothesis of a normal distribution is often shown in the output of the model. In our example, this assumption would be rejected (using a common significance level such as <span class="math notranslate nohighlight">\(0.01\)</span> or <span class="math notranslate nohighlight">\(0.05\)</span>). In principle, this is less problematic for the estimation of the parameters, but no statements should be made based on the normal distribution. The assumption of normal distribution is made relatively often in the literature for various models, although in reality it often has to be discarded for empirical data. Its nevertheless frequent use often has something to do with the mathematically pleasing properties of the normal distribution.</p>
<p><strong>Homoskedasticity</strong></p>
<p>The property of homoscedasticity arises from the assumption that the variance of all residuals is equal <span class="math notranslate nohighlight">\(\epsilon \sim N(0; \sigma^2)\)</span>, if this is not true, individual observations <span class="math notranslate nohighlight">\(i\)</span> exhibit different variances <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>. In the cell below, you can see an example of simulated data whose variance increases with the independent variable. The corresponding residual plot is particularly meaningful here, as it gives a good graphical indication that the variances of the predictions vary in different ways. The good news is that the estimated parameters of the regression line can be further trusted since they are consistently unbiased, but attention should be paid to statistical inference in the presence of heteroscedasticity. The standard errors of the estimators tend to be underestimated, which can lead to erroneous inferences. Options for dealing with heteroskedasticity include adjusting the standard errors or adjusted estimation procedures such as the weighted least squares approach. One may use different approaches to identify potential heteroskedasticity. A good way is to examine the residual plot. If you want you can add statistical tests like the Breusch-Pagan or White test. Given, heteroscedasticity, a difference in the standard errors will result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>


<span class="c1"># simulate data</span>
<span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span>
<span class="n">sigma_c</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
    <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()</span> <span class="o">*</span> <span class="n">sigma_c</span> <span class="o">*</span> <span class="mf">0.003</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">x</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># estimate model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># visualize residuals</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)),</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;observation number&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\epsilon$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fe76994a24ab25f9c046438624e7989f5a4cd170a649937676e7cd9e9095dafa.png" src="_images/fe76994a24ab25f9c046438624e7989f5a4cd170a649937676e7cd9e9095dafa.png" />
</div>
</div>
<p>First, take a look at the model output below, if we do not correct standard errors for heteroscedasticity. Especially, take a look at the standard error for <span class="math notranslate nohighlight">\(\hat{\beta}_1 = 0.014\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span>
    <span class="n">formula</span><span class="o">=</span><span class="s2">&quot;y ~ x&quot;</span><span class="p">,</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">df</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> <span class="c1">#, </span>

<span class="c1">#</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.152
Model:                            OLS   Adj. R-squared:                  0.141
Method:                 Least Squares   F-statistic:                     14.02
Date:                Tue, 28 Jan 2025   Prob (F-statistic):           0.000344
Time:                        13:53:27   Log-Likelihood:                 42.582
No. Observations:                  80   AIC:                            -81.16
Df Residuals:                      78   BIC:                            -76.40
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.9804      0.044     22.046      0.000       0.892       1.069
x              0.0522      0.014      3.744      0.000       0.024       0.080
==============================================================================
Omnibus:                       22.196   Durbin-Watson:                   2.166
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               56.686
Skew:                          -0.857   Prob(JB):                     4.91e-13
Kurtosis:                       6.750   Cond. No.                         9.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Now, watch at the output and the same standard error, if we include a correction for heteroscedasticity. The standard error increases from <span class="math notranslate nohighlight">\(0.014\)</span> to <span class="math notranslate nohighlight">\(0.018\)</span>, this reduces the <span class="math notranslate nohighlight">\(t\)</span> statistic which increases the p-value. In the example, one still would infer a significant impact of the independent variable, however, in reality, correcting for heteroscedasticity can lead to different inferences on the data. Note, that we do not discuss, how corrections are made, however, the potential impact should be clear to you.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span>
    <span class="n">formula</span><span class="o">=</span><span class="s2">&quot;y ~ x&quot;</span><span class="p">,</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">df</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s2">&quot;HC3&quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.152
Model:                            OLS   Adj. R-squared:                  0.141
Method:                 Least Squares   F-statistic:                     8.693
Date:                Tue, 28 Jan 2025   Prob (F-statistic):            0.00421
Time:                        13:53:27   Log-Likelihood:                 42.582
No. Observations:                  80   AIC:                            -81.16
Df Residuals:                      78   BIC:                            -76.40
Df Model:                           1                                         
Covariance Type:                  HC3                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.9804      0.040     24.560      0.000       0.902       1.059
x              0.0522      0.018      2.948      0.003       0.017       0.087
==============================================================================
Omnibus:                       22.196   Durbin-Watson:                   2.166
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               56.686
Skew:                          -0.857   Prob(JB):                     4.91e-13
Kurtosis:                       6.750   Cond. No.                         9.58
==============================================================================

Notes:
[1] Standard Errors are heteroscedasticity robust (HC3)
</pre></div>
</div>
</div>
</div>
<p><strong>Independent error terms</strong>.</p>
<p>The assumption of independent error terms implies that there are no systematic relationships between the residuals <span class="math notranslate nohighlight">\(\epsilon\)</span> of the individual observations. This assumption is often not met for time series data or clustered data. Similar to the presence of heteroskedasticity, the consequences primarily affect statistical inference, while the estimators of the regression lines remain asymptotically unbiased. Dealing with this can be done by adjusting the standard errors (in analogy to the proceeding above for heteroscedasticity) or by including adjusted models for the error terms.</p>
<p><strong>Multicollinearity</strong></p>
<p>Finally, it can become problematic for the estimation of the linear regression model if independent variables show (high) dependencies. Intuitively, it becomes difficult to identify the individual influences of these variables separately in this way. In the extreme case of perfect linear dependence, mathematical problems also arise that make estimation impossible. It is therefore always a good idea to look at the correlations of the independent variables in the first step. In the cell below you can see the correlation matrix for our example data set. In this case, the correlation between the two most important variables is relatively low, which is why the aspect of multicollinearity is not relevant for this data set. If high correlations are found, a first simple help could be to remove one of the highly correlated variables from the model. Methodologically, the variance-inflation factor can be used to quantify how pronounced multicollinearity is for the existing data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">advertising_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;sales&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>TV</th>
      <td>1.000000</td>
      <td>0.054809</td>
      <td>0.056648</td>
    </tr>
    <tr>
      <th>radio</th>
      <td>0.054809</td>
      <td>1.000000</td>
      <td>0.354104</td>
    </tr>
    <tr>
      <th>newspaper</th>
      <td>0.056648</td>
      <td>0.354104</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="time-series-and-cross-sectional-regressions-of-asset-returns">
<h2>Time-series and cross sectional regressions of asset returns<a class="headerlink" href="#time-series-and-cross-sectional-regressions-of-asset-returns" title="Link to this heading">#</a></h2>
<p>Linear regression is often used as a tool for analysis in economics and finance. Two very popular examples for the usage of regression models is given by time-series and cross sectional regressions of asset returns.</p>
<section id="time-series-regressions">
<h3>Time-series regressions<a class="headerlink" href="#time-series-regressions" title="Link to this heading">#</a></h3>
<p>Time-series regressions can often be found in the context of factor models. A factor model regresses an asset return <span class="math notranslate nohighlight">\(r_{t, j}\)</span> of asset <span class="math notranslate nohighlight">\(j\)</span> upon financial factors <span class="math notranslate nohighlight">\(f_{t, 1}, f_{t, 2}, ..., f_{t, q}\)</span>. One of the most popular factor models is the Capital Asset Pricing Model (CAPM) which is derived based upon Markowitz portfolio theory and has been developed independently by several researchers in the 1960s. First, let us take a look at some background information. In the center of the CAPM is the security market line which illustrates the relationship between the expected excess return of asset <span class="math notranslate nohighlight">\(j\)</span> to the excess return of the market portfolio. The latter is the efficient portfolio every rational investor allocates its wealth under certain assumptions. The corresponding formula is:</p>
<div class="math notranslate nohighlight">
\[
\mu_j - r_f = \beta_j \left(\mu_M - r_f \right)
\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu_j\)</span>: the expected return of asset <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_j\)</span>: the expected return of the market portfolio <span class="math notranslate nohighlight">\(M\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(r_f\)</span>: the risk free rate of return</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_j = \frac{\sigma_{jM}}{\sigma_M^2}\)</span>: the sensitivity of asset return <span class="math notranslate nohighlight">\(j\)</span> to the market portfolio; <span class="math notranslate nohighlight">\(\sigma_{jM}\)</span> is the covariance between asset return <span class="math notranslate nohighlight">\(j\)</span> and the market portfolio, while <span class="math notranslate nohighlight">\(\sigma_M^2\)</span> represents the market portfolio’s variance</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_j - r_f\)</span> and <span class="math notranslate nohighlight">\(\mu_M - r_f\)</span>: risk premium of asset <span class="math notranslate nohighlight">\(j\)</span> and the market portfolio</p></li>
</ul>
<p>The security market line tells us that the risk premium for asset <span class="math notranslate nohighlight">\(j\)</span> is higher if the asset’s returns react more sensitive to the market development. Multiple angles can be taken from that point of view. First, let us take a look at the data generating process behind the security market line which is called the security characteristic line:</p>
<div class="math notranslate nohighlight">
\[
r_{t, j} = r_{t, f} + \beta_j \left(r_{t, m} - r_{t, f} \right) + \epsilon_{t, j}
\]</div>
<p>Basically, the security market line is the expected value for the security characteristic line. For simplicity let us denote <span class="math notranslate nohighlight">\(\tilde{r}_{t, j} = r_{t, j} - r_{t, f}\)</span> and <span class="math notranslate nohighlight">\(\tilde{r}_{t, M} = r_{t, M} - r_{t, f}\)</span>, respectively.</p>
<p><em>High betas increase a company’s risk:</em></p>
<p>The variance of asset return <span class="math notranslate nohighlight">\(j\)</span> (<span class="math notranslate nohighlight">\(\sigma_{j}\)</span>) can be decomposed to:</p>
<div class="math notranslate nohighlight">
\[
\sigma_{j}^2 = \beta_j^2 \sigma_{M}^2 + \sigma_{\epsilon}^2
\]</div>
<p>The first part <span class="math notranslate nohighlight">\(\beta_j^2 \sigma_{M}^2\)</span> is called systematic risk while <span class="math notranslate nohighlight">\(\sigma_{\epsilon}^2\)</span> is called unsystematic or idiosyncratic risk. It can be shown that the latter can be reduced by diversification, however, systematic risk can not. The higher an assets’s beta, the higher the systematic risk, and given equal idiosyncratic risk, the higher the asset’s risk (when measured by variance).</p>
<p><em>Assets with high betas exhibit strong linear relationships:</em></p>
<p>The covariance between two assets <span class="math notranslate nohighlight">\(k, l\)</span> whose (excess) returns follow the data generating process of the security characteristic line is given by:</p>
<div class="math notranslate nohighlight">
\[
\sigma_{kl} = \beta_k \beta_l \sigma_M^2
\]</div>
<p><em>Assets with high betas increase the risk of the market portfolio:</em></p>
<p>Given a market with <span class="math notranslate nohighlight">\(N\)</span> assets with weights <span class="math notranslate nohighlight">\(w_{M, 1}, ..., w_{M, N}\)</span>, then the market portfolio return is: <span class="math notranslate nohighlight">\(r_{t, M} = \sum_i w_{M, i} r_{t, i}\)</span>. The covariance <span class="math notranslate nohighlight">\(\sigma_{j, M}\)</span> between an asset return <span class="math notranslate nohighlight">\(r_{t, k}\)</span> and the market portfolio return is given by:</p>
<div class="math notranslate nohighlight">
\[
\sigma_{j, M} = \sum_i w_{M, i} \sigma_{ij}
\]</div>
<p>Thus, the market portfolio’s variance (risk) is:</p>
<div class="math notranslate nohighlight">
\[
\sigma_M^2 = \sum_j \sum_i w_{M, j} w_{M, i} \sigma{ij} = \sum_j w_{M, j} \sum_i w_{M, i} \sigma_{ij} = \sum_j w_{M, j} \sigma_{M, j}
\]</div>
<p>This shows that assets with a higher covariance to the market portfolio <span class="math notranslate nohighlight">\(\sigma_{M, j}\)</span> (which also means that they have a higher beta), increase the portfolio’s risk to a larger extent.</p>
<p>The implications of these relationships are:</p>
<ul class="simple">
<li><p>Assets with high betas should have a higher risk premium</p></li>
<li><p>Assets with high betas come along with higher non-diversifiable (systemtic) risk, this is the part of risk which matters to diversified investors</p></li>
<li><p>Assets with high betas exhibit stronger linear dependence to other assets</p></li>
<li><p>Assets with high betas increase the market portfolio’s risk</p></li>
</ul>
<p>For each asset <span class="math notranslate nohighlight">\(\beta_j\)</span> can be estimated by the security characteristic line:</p>
<div class="math notranslate nohighlight">
\[
\tilde{r}_{t, j} = \beta_j \tilde{r}_{t, M} + \epsilon_{t, j}
\]</div>
<p>This is a linear regression model without an intercept. The intercept should be equal to zero, if the assumptions of the CAPM hold, however, we can further include an intercept in the regression to allow mis-pricing.</p>
<div class="math notranslate nohighlight">
\[
\tilde{r}_{t, j} = \alpha + \beta_j \tilde{r}_{t, M} + \epsilon_{t, j}
\]</div>
<p>In this context, the intercept is often denoted by <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please note that all derivations of the model itself are not in the scope of this course. Furthermore, to understand and derive the formulas for the expected value, the variance and covariance of asset returns as shown above, one needs to have background knowledge how to determine the expected value, the variance and covariance of and between linear combinations of random variables.</p>
</div>
<p>The cell below exhibits regression results for daily excess returns of Apple from the last two years. We use the market portfolio provided by <a class="reference external" href="https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html">Kenneth French’s data library</a>. The beta coefficient is higher than <span class="math notranslate nohighlight">\(1\)</span>, indicating a sensitive co-movement of Apple’s return and the market. The adjusted coefficient of determination is around <span class="math notranslate nohighlight">\(45\%\)</span>, thus, a significant part of the returns variability can be explained by its relation to the market.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sqlite3</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;../data/sp_returns.sqlite&quot;</span><span class="p">)</span>
<span class="n">ff_daily</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_sql</span><span class="p">(</span><span class="s2">&quot;Select * From ff_daily;&quot;</span><span class="p">,</span> <span class="n">conn</span><span class="p">)</span>
<span class="n">df_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_sql</span><span class="p">(</span><span class="s2">&quot;Select * From daily;&quot;</span><span class="p">,</span> <span class="n">conn</span><span class="p">)</span>
<span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">ff_daily</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">ff_daily</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">])</span>
<span class="n">ff_daily</span> <span class="o">=</span> <span class="n">ff_daily</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="s2">&quot;date&quot;</span><span class="p">:</span> <span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="s2">&quot;Mkt-RF&quot;</span><span class="p">:</span> <span class="s2">&quot;r_m_exs&quot;</span><span class="p">,</span> <span class="s2">&quot;SMB&quot;</span><span class="p">:</span> <span class="s2">&quot;smb&quot;</span><span class="p">,</span> <span class="s2">&quot;HML&quot;</span><span class="p">:</span> <span class="s2">&quot;hml&quot;</span><span class="p">,</span> <span class="s2">&quot;RF&quot;</span><span class="p">:</span> <span class="s2">&quot;r_f&quot;</span><span class="p">},</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">df_returns</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df_returns</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">tz_localize</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

<span class="n">df_tmp</span> <span class="o">=</span> <span class="n">df_returns</span><span class="p">[</span><span class="n">df_returns</span><span class="o">.</span><span class="n">Symbol</span> <span class="o">==</span> <span class="s2">&quot;AAPL&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_regression</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_tmp</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="s2">&quot;r_t&quot;</span><span class="p">]),</span> <span class="n">ff_daily</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="s2">&quot;Date&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;r_m_exs&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;r_t&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;r_f&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.479
Model:                            OLS   Adj. R-squared:                  0.478
Method:                 Least Squares   F-statistic:                     421.1
Date:                Tue, 28 Jan 2025   Prob (F-statistic):           7.69e-67
Time:                        13:53:28   Log-Likelihood:                 1424.1
No. Observations:                 460   AIC:                            -2844.
Df Residuals:                     458   BIC:                            -2836.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2.844e-05      0.001      0.055      0.956      -0.001       0.001
r_m_exs        1.1318      0.055     20.520      0.000       1.023       1.240
==============================================================================
Omnibus:                       75.150   Durbin-Watson:                   1.896
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              514.536
Skew:                           0.471   Prob(JB):                    1.86e-112
Kurtosis:                       8.095   Cond. No.                         108.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>To get an idea about the range of beta coefficients and the level of variability explained by the market, we conduct the regression from above for all companies in the S&amp;P 500, S&amp;P 400 and S&amp;P 600, respectively. This includes large, mid-sized and small companies. Furthermore, we also collect sector information according to the GICS standard. The boxplots below exhibit the range of beta coefficients grouped by sector and index, respectively.</p>
<p>We observe variation of beta coefficients between sectors, e.g., companies from the Information Technology sector seems to have higher beta coefficients which higher systematic risk for this sector. Between indices, we examine higher beta levels for smaller companies (S&amp;P 600), however, the level of variation seems not to be as large as among sectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">regression_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="n">df_returns</span><span class="o">.</span><span class="n">Symbol</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;p_value_alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;R2_adj&quot;</span><span class="p">,</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="s2">&quot;Index&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">ticker</span><span class="p">,</span> <span class="n">df_tmp</span> <span class="ow">in</span> <span class="n">df_returns</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;Symbol&quot;</span><span class="p">):</span>
    <span class="n">df_regression</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_tmp</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="s2">&quot;r_t&quot;</span><span class="p">]),</span> <span class="n">ff_daily</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="s2">&quot;Date&quot;</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;r_m_exs&quot;</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;r_t&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;r_f&quot;</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">regression_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ticker</span><span class="p">,</span> <span class="p">:</span><span class="s2">&quot;R2_adj&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">results</span><span class="o">.</span><span class="n">pvalues</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">results</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="p">]</span>
    <span class="n">regression_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ticker</span><span class="p">,</span> <span class="s2">&quot;Sector&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_tmp</span><span class="p">[</span><span class="s2">&quot;GICS Sector&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">regression_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ticker</span><span class="p">,</span> <span class="s2">&quot;Index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_tmp</span><span class="p">[</span><span class="s2">&quot;index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Index&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Betas by Sector&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Betas by Index&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5c4976e3667cee515db1fb220fc0b9e7d14d47b3b2aae8c3f8e89b89baf40f1f.png" src="_images/5c4976e3667cee515db1fb220fc0b9e7d14d47b3b2aae8c3f8e89b89baf40f1f.png" />
</div>
</div>
<p>The next cell exhibits levels for the adjusted <span class="math notranslate nohighlight">\(R^2\)</span>. Again, the variation among sectors seems to be higher in comparison to the indices (and thus firm sizes). While a few companies exist whose return’s variation can not be explained by the market at all, the average level is around <span class="math notranslate nohighlight">\(0.25\)</span> with individual values exceeding <span class="math notranslate nohighlight">\(0.50\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;R2_adj&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;R2_adj&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Index&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;R2 (adj) by Sector&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;R2 (adj) by Index&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/21e7e6643ed72f9b34ef01116a48920faeec0727f6d1850f40fe32749fa74c0e.png" src="_images/21e7e6643ed72f9b34ef01116a48920faeec0727f6d1850f40fe32749fa74c0e.png" />
</div>
</div>
<p>Empirical studies revealed that the market portfolio (often also called market factor in this context) seems not to be sufficient to explain asset premium and, thus, returns variation can be better explained when adding more factors to the equation which are build around variables that are fit to differentiate excess returns of companies. Two very prominent examples are the size and book-to-value factors, extending the original model to the Fama-French three factor model. Detailed explanations of the factor constructions can be found <a class="reference external" href="https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_factors.html">here</a>, but in short: the small minus big (SMB) factor is similar to a portfolio which invests into smaller companies (according to their market capitalization) and short sells large companies; the high minus low (HML) factor is a portfolio which invests into companies with high ratios of the book-to-market value and short sells companies with low book-to-market values. The book-to-market value compares the book value of a company derived by accounting standards to the market value which captures the investor’s belief about the (future) value of the company. Thus, companies with low book-to-market values can indicate high growth potential as the majority of investors seems to believe the value of the company should be higher than its current book value. The regression which is used to estimate the betas for these factors is given by:</p>
<div class="math notranslate nohighlight">
\[
\tilde{r}_{t, j} = \alpha + \beta_{M, j} \tilde{r}_{t, M} + \beta_{SMB, j} r_{t, SMB} + \beta_{HML, j} r_{t, HML} + \epsilon_{t, j}
\]</div>
<p>Values for <span class="math notranslate nohighlight">\(\beta_{SMB, j}\)</span> indicate if a company’s returns rather are in line with the average development of small or large companies, if <span class="math notranslate nohighlight">\(\beta_{SMB, j} &gt; 0\)</span>, the company’s development is in line with the development of smaller companies and vice versa. Values for <span class="math notranslate nohighlight">\(\beta_{HML, j}\)</span> indicate if a company’s returns rather are in line with the average development of companies which either have a larger or smaller book-to-market ratio, if <span class="math notranslate nohighlight">\(\beta_{HML, j} &gt; 0\)</span>, the company’s development is in line with the development of companies with a higher book-to-market value and vice versa.</p>
<p>Below we observe that the vast majority of companies’ returns are more in line with the ones of small companies. Furthermore, the Financials sector is one with the highest <span class="math notranslate nohighlight">\(\beta_{HML, j}\)</span> values representing companies which are often referred as value stocks. These companies usually tend to exist for a longer time and are more “settled” in terms of their firm value and the market’s perception of it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">regression_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="n">df_returns</span><span class="o">.</span><span class="n">Symbol</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;beta_m&quot;</span><span class="p">,</span> <span class="s2">&quot;beta_smb&quot;</span><span class="p">,</span> <span class="s2">&quot;beta_hml&quot;</span><span class="p">,</span> <span class="s2">&quot;p_value_alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;R2_adj&quot;</span><span class="p">,</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="s2">&quot;Index&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">ticker</span><span class="p">,</span> <span class="n">df_tmp</span> <span class="ow">in</span> <span class="n">df_returns</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;Symbol&quot;</span><span class="p">):</span>
    <span class="n">df_regression</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_tmp</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="s2">&quot;r_t&quot;</span><span class="p">]),</span> <span class="n">ff_daily</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="s2">&quot;Date&quot;</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;r_m_exs&quot;</span><span class="p">,</span> <span class="s2">&quot;smb&quot;</span><span class="p">,</span> <span class="s2">&quot;hml&quot;</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;r_t&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;r_f&quot;</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">regression_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ticker</span><span class="p">,</span> <span class="p">:</span><span class="s2">&quot;R2_adj&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">results</span><span class="o">.</span><span class="n">pvalues</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">results</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="p">]</span>
    <span class="n">regression_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ticker</span><span class="p">,</span> <span class="s2">&quot;Sector&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_tmp</span><span class="p">[</span><span class="s2">&quot;GICS Sector&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">regression_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ticker</span><span class="p">,</span> <span class="s2">&quot;Index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_tmp</span><span class="p">[</span><span class="s2">&quot;index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta_m&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta_smb&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta_hml&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta_m&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Index&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta_smb&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Index&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;beta_hml&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Index&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b899824483ade5de788c60f14929f5e44e08f1e529ae99e876503a5f251ac228.png" src="_images/b899824483ade5de788c60f14929f5e44e08f1e529ae99e876503a5f251ac228.png" />
</div>
</div>
<p>Taking a look at the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> values, we observe that a larger level of the returns’ variation can be explained by including the size and value factors. This underlines empirical findings which speak against the CAPM and the existence of factors which impact the asset risk premium beyond the covariance to the market.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;R2_adj&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Sector&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">regression_results</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;R2_adj&quot;</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;Index&quot;</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;R2 (adj) by Sector&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;R2 (adj) by Index&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6a76fdf3634a71568da3bcb6e3f099cdf60110e0729dc65d9426788d22634f6c.png" src="_images/6a76fdf3634a71568da3bcb6e3f099cdf60110e0729dc65d9426788d22634f6c.png" />
</div>
</div>
</section>
</section>
<section id="cross-sectional-regressions-of-asset-return">
<h2>Cross sectional regressions of asset return<a class="headerlink" href="#cross-sectional-regressions-of-asset-return" title="Link to this heading">#</a></h2>
<p>The addition of a SMB and BTM factor to the time-series regression is related to findings which stem from cross sectional regressions in the previous literature. These cross sectional regressions aim to identify which firm variables impact asset returns risk premiums. The foundation of empirical studies in this context are regression of the form:</p>
<div class="math notranslate nohighlight">
\[
\tilde{r}_{j, t + 1} = \alpha_i + \beta_{t}^{f_1} f_{j1, t} + \beta_{t}^{f_2} f_{j2, t} + ... + \beta_{t}^{f_p} f_{jp, t} + \epsilon_{j, t}
\]</div>
<p>The main differences are that the regression <span class="math notranslate nohighlight">\(\beta\)</span>-s are time specific and the factor observations are all at the same point in time. The former can be identified by the <span class="math notranslate nohighlight">\(t\)</span> in the index of each <span class="math notranslate nohighlight">\(\beta\)</span>. To keep the notation simple, let us take a look at a specific example. For a month <span class="math notranslate nohighlight">\(t+1\)</span>, we regress the excess return of each company <span class="math notranslate nohighlight">\(j\)</span> on three variables at time <span class="math notranslate nohighlight">\(t\)</span>, i.e., the log-market-cap (<span class="math notranslate nohighlight">\(Size_t\)</span>), the book-to-market-value (<span class="math notranslate nohighlight">\(BTM_t\)</span>) and the six month momentum of each company (<span class="math notranslate nohighlight">\(MoM_t\)</span>). The latter is simply the percentage change in the stock price over the past six month. The regression is:</p>
<div class="math notranslate nohighlight">
\[
\tilde{r}_{j, t + 1} = \alpha_i + \beta_{t}^{Size} Size_{j, t} + \beta_{t}^{BTM} BTM_{j, t} + \beta_{t}^{MoM} MoM_{j, t} + \epsilon_{j, t}
\]</div>
<p>The data sample for our analysis includes US stock listed companies from major stock market indices. The sample includes monthly observations with the number of firm observations per month being approximately between <span class="math notranslate nohighlight">\(2,200\)</span> and <span class="math notranslate nohighlight">\(2,600\)</span>. Overall this data sample includes <span class="math notranslate nohighlight">\(619,110\)</span> observations in the time period between 2002 and 2024. The table below gives a descriptive overview of all observations over time after each variable has been winsorized at the <span class="math notranslate nohighlight">\(0.01\)</span> and <span class="math notranslate nohighlight">\(0.99\)</span> level.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df_regression</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/fama_macbeth_cross_sectional_data.csv&quot;</span><span class="p">)</span>
<span class="n">df_regression</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df_regression</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">])</span>
<span class="n">df_regression</span> <span class="o">=</span> <span class="n">df_regression</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;Date&quot;</span><span class="p">)</span>
<span class="n">df_regression</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">])</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ex_rt_shifted</th>
      <th>mom_6</th>
      <th>Size</th>
      <th>BTM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>0.010432</td>
      <td>0.063295</td>
      <td>21.294131</td>
      <td>0.626844</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.132852</td>
      <td>0.373159</td>
      <td>1.777521</td>
      <td>0.891342</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-0.737878</td>
      <td>-0.938606</td>
      <td>15.637645</td>
      <td>0.005471</td>
    </tr>
    <tr>
      <th>1%</th>
      <td>-0.329080</td>
      <td>-0.675947</td>
      <td>17.199075</td>
      <td>0.025466</td>
    </tr>
    <tr>
      <th>5%</th>
      <td>-0.194492</td>
      <td>-0.451923</td>
      <td>18.412886</td>
      <td>0.071814</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.007096</td>
      <td>0.033692</td>
      <td>21.231659</td>
      <td>0.424327</td>
    </tr>
    <tr>
      <th>95%</th>
      <td>0.223833</td>
      <td>0.652071</td>
      <td>24.389604</td>
      <td>1.672808</td>
    </tr>
    <tr>
      <th>99%</th>
      <td>0.416073</td>
      <td>1.330314</td>
      <td>25.570209</td>
      <td>4.407628</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.443653</td>
      <td>5.544949</td>
      <td>26.527139</td>
      <td>24.933013</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If we conduct the cross sectional regression in our data sample, the following results can be observed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>

<span class="n">dt</span><span class="p">,</span> <span class="n">df_tmp</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">df_regression</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;Date&quot;</span><span class="p">)))</span>

<span class="n">one_month_regression</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span>
    <span class="n">formula</span><span class="o">=</span><span class="s2">&quot;ex_rt_shifted ~ mom_6 + Size + BTM&quot;</span><span class="p">,</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">df_tmp</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">one_month_regression</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:          ex_rt_shifted   R-squared:                       0.005
Model:                            OLS   Adj. R-squared:                  0.004
Method:                 Least Squares   F-statistic:                     4.015
Date:                Tue, 28 Jan 2025   Prob (F-statistic):            0.00733
Time:                        13:53:32   Log-Likelihood:                 1370.8
No. Observations:                2300   AIC:                            -2734.
Df Residuals:                    2296   BIC:                            -2711.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0049      0.035     -0.142      0.887      -0.073       0.063
mom_6          0.0086      0.010      0.897      0.370      -0.010       0.027
Size           0.0003      0.002      0.173      0.863      -0.003       0.004
BTM            0.0111      0.003      3.372      0.001       0.005       0.018
==============================================================================
Omnibus:                      162.121   Durbin-Watson:                   1.961
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              406.211
Skew:                           0.406   Prob(JB):                     6.20e-89
Kurtosis:                       4.892   Cond. No.                         256.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>In our example the <span class="math notranslate nohighlight">\(BTM\)</span> variable is the only one which seems to have an impact that is statistically different from zero. However for our data set, we have 265 monthly data samples. For each of these monthly samples, the cross sectional regression from above is repeated. Due to the statistical uncertainy and general aspects of hypothesis test, the estimated regression parameters for an individual month are not conclusive regarding the general question if one of the variables has a significant impact on the next month’s stock return. This is why for every month <span class="math notranslate nohighlight">\(t = 1, ..., T\)</span>, the cross sectional regression is conducted. Over time, the average effect of a variable can be determined by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}^{f_i} = \frac{1}{T} \sum_{t = 1}^T \hat{\beta}_{t}^{f_i}
\]</div>
<p>Under the assumption of normality or given a large number of time steps (such that the central limit theorem implies normality), <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> is a normally distributed random variable with variance:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}_{\hat{\beta}_{j}}^2 = \frac{1}{T(T-1)} \sum_{t = 1}^T \left( \hat{\beta}_{t}^{f_i} - \hat{\beta}^{f_i} \right)^2
\]</div>
<p>Thus, the corresponding t-statistic is given by:</p>
<div class="math notranslate nohighlight">
\[
t_{\hat{\beta^{f_i}}} = \frac{\hat{\beta}^{f_i}}{\sigma_{\hat{\beta}^{f_i}}}
\]</div>
<p>The p-value can be determined by: <span class="math notranslate nohighlight">\(P\left(T \geq |t_{\hat{\beta^{f_i}}}| \right) \cdot 2\)</span>. However, in most of these studies, the t-value itself is interpreted. Usually, absolute values higher than <span class="math notranslate nohighlight">\(1.65\)</span> are an indication of for the value being different from zero. If this is the case, the inference is that the corresponding variable has a significant impact on a company’s (future) returns. Investors demand higher return for more risk. For instance, an empirical finding is a so called size-premium which is identified by <span class="math notranslate nohighlight">\(\hat{\beta}^{Size} &lt; 0\)</span> and signaling that larger (smaller) companies have a smaller (larger) expected return. An economic explanation for this premium may have its origin that smaller companies are less liquid, have higher bankruptcy, etc.</p>
<p>Now, let us take a look at the results for our data set, below we observe estimated model parameters over time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>


<span class="n">risk_premiums</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">df_regression</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;Date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">df_tmp</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span>
            <span class="n">formula</span><span class="o">=</span><span class="s2">&quot;ex_rt_shifted ~ mom_6 + Size + BTM&quot;</span><span class="p">,</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">df_tmp</span>
        <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">params</span>  
    <span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="p">)</span>


<span class="n">risk_premiums</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;Date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subplots</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f8c35f85e299244afdace259842dc54c4df93ec8c42fef9b01b6f6010871b83f.png" src="_images/f8c35f85e299244afdace259842dc54c4df93ec8c42fef9b01b6f6010871b83f.png" />
</div>
</div>
<p>If we calculate t-statistics with and without the correction of heteroscedasticity and auto-correlation, the following results can be observed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">price_of_risk</span> <span class="o">=</span> <span class="p">(</span><span class="n">risk_premiums</span>
  <span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;factor&quot;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s2">&quot;estimate&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;factor&quot;</span><span class="p">)[</span><span class="s2">&quot;estimate&quot;</span><span class="p">]</span>
  <span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">({</span>
      <span class="s2">&quot;risk_premium&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
      <span class="s2">&quot;t_statistic&quot;</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="p">})</span>
  <span class="p">)</span>
  <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
  <span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;factor&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;level_1&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;estimate&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">price_of_risk_newey_west</span> <span class="o">=</span> <span class="p">(</span><span class="n">risk_premiums</span>
  <span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;factor&quot;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s2">&quot;estimate&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;factor&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span>
      <span class="n">x</span><span class="p">[</span><span class="s2">&quot;estimate&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span> 
        <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;estimate ~ 1&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s2">&quot;HAC&quot;</span><span class="p">,</span> <span class="n">cov_kwds</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;maxlags&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">})</span><span class="o">.</span><span class="n">bse</span>
    <span class="p">),</span> <span class="n">include_groups</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="p">)</span>
  <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
  <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Intercept&quot;</span><span class="p">:</span> <span class="s2">&quot;t_statistic_newey_west&quot;</span><span class="p">})</span>
<span class="p">)</span>

<span class="p">(</span><span class="n">price_of_risk</span>
  <span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">price_of_risk_newey_west</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;factor&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>factor</th>
      <th>risk_premium</th>
      <th>t_statistic</th>
      <th>t_statistic_newey_west</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>BTM</td>
      <td>0.022</td>
      <td>0.231</td>
      <td>0.169</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Intercept</td>
      <td>2.050</td>
      <td>1.535</td>
      <td>1.223</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Size</td>
      <td>-0.061</td>
      <td>-1.176</td>
      <td>-0.900</td>
    </tr>
    <tr>
      <th>3</th>
      <td>mom_6</td>
      <td>0.389</td>
      <td>1.099</td>
      <td>1.189</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This means that, we would not find any of the variables to have a significant impact on the asset return. However, signs of each variable is in line with empirical findings in the literature, i.e.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}^{Size} &lt; 0\)</span>: Smaller companies have higher expected returns</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}^{BTM} &gt; 0\)</span>: Values with higher book-to-market returns have higher expected returns</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}^{Mom} &gt; 0\)</span>: Companies with a positive momentum have higher expected returns</p></li>
</ul>
<p>The reason for the effects not being statistically different from zero may also be linked to the relative short time period which causes the number of months and estimates to be relatively small. If you are interested in this topic a great online book is available <a class="reference external" href="https://www.tidy-finance.org/python/">here</a>.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Even though the linear regression model probably makes highly simplifying assumptions for reality in many cases, didactically it is a very good introduction to dependent variable modeling. We have looked at and discussed many details in more detail in this chapter, which we do retain for the upcoming models. However, it is true that many basic ideas, such as the mental distinction of the model and the estimation of the conditional expected value, exist in the same way for other and sometimes much more complex models. The most important aspects of this chapter for this course are:</p>
<ul class="simple">
<li><p>how is the model defined - from this it can be inferred what type of relationship is assumed between the dependent and independent variables</p></li>
<li><p>how are the parameters of the model estimated - the use of a loss function that is minimized as the parameters change is very common for many different models</p></li>
<li><p>what inferences can be made from the estimated model - how well can the model explain the data, what variables are important, what is their influence</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_SupervisedLearning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The analysis of dependent variables</p>
      </div>
    </a>
    <a class="right-next"
       href="06_Classification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-linear-regression-model">Training the linear regression model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-uncertainty">Estimation uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-quality">Model quality</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessment-of-the-independent-variables">Assessment of the independent variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">Variable selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deviations-of-the-model-assumptions">Deviations of the model assumptions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-and-cross-sectional-regressions-of-asset-returns">Time-series and cross sectional regressions of asset returns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-regressions">Time-series regressions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-sectional-regressions-of-asset-return">Cross sectional regressions of asset return</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>